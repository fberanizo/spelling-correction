{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BEA2019_BERT_PT.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fberanizo/spelling-correction/blob/master/BEA2019_BERT_PT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Up4u8rMkQSG",
        "colab_type": "text"
      },
      "source": [
        "![alt text](https://)# Grammatical Error Correction: Modelo BERT PT\n",
        "\n",
        "**Nome: Fabio Beranizo Lopes**<br>\n",
        "**Nome: Luiz Pita Almeida**\n",
        "\n",
        "Usaremos o modelo BERT pré-treinado e o dataset  BrWaC (Brazilian Web as Corpus) <br>\n",
        "\n",
        "Métrica de avaliação: F0.5-score <br>\n",
        "https://www.cl.cam.ac.uk/research/nl/bea2019st/#eval\n",
        "\n",
        "O método de correção aplicado foi sugerido pelos docentes:<br>\n",
        "> dado uma frase e um palavra nesta frase a ser corrigida ou não, iremos\n",
        "> mascarar a palavra, rodar o BERT ou T5, e prever as top-10 palavras \n",
        "> alternativas usando mask language modeling. Se a palavra original estiver \n",
        "> entre as top previstas, não sugerir correção. Caso contrário, usar edit \n",
        "> distance para ver qual é a palavra mais próxima, e sugerí-la ao usuário.\n",
        "\n",
        "Passos:\n",
        "\n",
        "1. Geram-se tuplas: `(input_ids, lm_labels, original, corrected)`\n",
        "2. Aplica-se modelo BERT para prever top-10 palavras.<br>\n",
        "   Caso a palavra original esteja no Top 10 do modelo, é classificada como correta.<br>\n",
        "   Senão, a palavra é classificada como incorreta.\n",
        "\n",
        "**Obs: os notebooks contém excertos de códigos dos colegas de turma.**<br>\n",
        "**Obrigado Diedre, Gabriela, Leard, Lucas e Israel.**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CpELBvNmku5a",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0a2d6c38-e1a9-4afa-ff35-cb1234337724"
      },
      "source": [
        "import torch\n",
        "\n",
        "print(f\"Current GPU: {torch.cuda.get_device_name(0)}\")\n",
        "\n",
        "# don't even start if it's not a P100 GPU\n",
        "# if torch.cuda.get_device_name(0) != \"Tesla P100-PCIE-16GB\":\n",
        "#     import os\n",
        "#     os.kill(os.getpid(), 9)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Current GPU: Tesla P100-PCIE-16GB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FgW-boJLU0wU",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "#@title Configurações gerais\n",
        "experiment_name = \"first\"\n",
        "model_name = \"bert-base-pt-cased\"  #@param [\"bert-base-pt-cased\", \"bert-large-pt-cased\"] {type:\"string\"}\n",
        "batch_size = 25  #@param {type:\"integer\"}\n",
        "accumulate_grad_batches = 10  #@param {type:\"integer\"}\n",
        "source_max_length = 50  #@param {type:\"integer\"}\n",
        "target_max_length = 100  #@param {type:\"integer\"}\n",
        "learning_rate = 5e-3  #@param {type:\"number\"}\n",
        "decode_mode = \"topk\"  #@param [\"greedy\", \"nucleus\", \"topk\", \"beam\"] {type:\"string\"}\n",
        "k = 10  #@param {type:\"integer\"}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1U0dTSB-mnGN",
        "colab_type": "text"
      },
      "source": [
        "## Instala dependências\n",
        "\n",
        "- PyTorch Lightning\n",
        "- Hugginface Transformers\n",
        "- ERRANT (ERRor ANnotation Toolkit)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OOXZxjWRkLMM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "20a588b4-0db8-4c13-c905-d730a4336f82"
      },
      "source": [
        "try:\n",
        "    import pytorch_lightning\n",
        "    import transformers\n",
        "except ImportError as e:\n",
        "    # can't import modules, then install\n",
        "    !pip install --quiet pytorch-lightning\n",
        "    !pip install --quiet transformers\n",
        "    !pip install --quiet errant==2.0.0\n",
        "    !python -m spacy download en\n",
        "    # kill kernel (necessary for tqdm)\n",
        "    import os\n",
        "    os.kill(os.getpid(), 9)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 296kB 2.9MB/s \n",
            "\u001b[K     |████████████████████████████████| 276kB 7.9MB/s \n",
            "\u001b[K     |████████████████████████████████| 829kB 9.0MB/s \n",
            "\u001b[?25h  Building wheel for PyYAML (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for future (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[K     |████████████████████████████████| 675kB 2.9MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.8MB 13.6MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.1MB 33.1MB/s \n",
            "\u001b[K     |████████████████████████████████| 890kB 38.4MB/s \n",
            "\u001b[?25h  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[K     |████████████████████████████████| 501kB 2.8MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.4MB 13.9MB/s \n",
            "\u001b[K     |████████████████████████████████| 81kB 9.3MB/s \n",
            "\u001b[K     |████████████████████████████████| 931kB 33.3MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.4MB 41.0MB/s \n",
            "\u001b[K     |████████████████████████████████| 184kB 44.1MB/s \n",
            "\u001b[K     |████████████████████████████████| 153kB 42.4MB/s \n",
            "\u001b[K     |████████████████████████████████| 614kB 23.2MB/s \n",
            "\u001b[K     |████████████████████████████████| 51kB 3.5MB/s \n",
            "\u001b[K     |████████████████████████████████| 389kB 37.8MB/s \n",
            "\u001b[?25h  Building wheel for spacy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for murmurhash (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for thinc (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for dill (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for regex (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for ftfy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for cytoolz (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: multiprocess 0.70.9 has requirement dill>=0.3.1, but you'll have dill 0.2.9 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: fastai 1.0.61 has requirement spacy>=2.0.18; python_version < \"3.8\", but you'll have spacy 1.9.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: en-core-web-sm 2.2.5 has requirement spacy>=2.2.2, but you'll have spacy 1.9.0 which is incompatible.\u001b[0m\n",
            "\n",
            "    Downloading en_core_web_sm-1.2.0/en_core_web_sm-1.2.0.tar.gz\n",
            "\n",
            "Collecting https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-1.2.0/en_core_web_sm-1.2.0.tar.gz\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-1.2.0/en_core_web_sm-1.2.0.tar.gz (52.2MB)\n",
            "\u001b[K    100% |████████████████████████████████| 52.2MB 59.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy<2.0.0,>=1.7.0 in /usr/local/lib/python3.6/dist-packages (from en-core-web-sm==1.2.0)\n",
            "Requirement already satisfied: preshed<2.0.0,>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from spacy<2.0.0,>=1.7.0->en-core-web-sm==1.2.0)\n",
            "Requirement already satisfied: ujson>=1.35 in /usr/local/lib/python3.6/dist-packages (from spacy<2.0.0,>=1.7.0->en-core-web-sm==1.2.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from spacy<2.0.0,>=1.7.0->en-core-web-sm==1.2.0)\n",
            "Requirement already satisfied: thinc<6.6.0,>=6.5.0 in /usr/local/lib/python3.6/dist-packages (from spacy<2.0.0,>=1.7.0->en-core-web-sm==1.2.0)\n",
            "Requirement already satisfied: pathlib in /usr/local/lib/python3.6/dist-packages (from spacy<2.0.0,>=1.7.0->en-core-web-sm==1.2.0)\n",
            "Requirement already satisfied: ftfy<5.0.0,>=4.4.2 in /usr/local/lib/python3.6/dist-packages (from spacy<2.0.0,>=1.7.0->en-core-web-sm==1.2.0)\n",
            "Requirement already satisfied: regex<2017.12.1,>=2017.4.1 in /usr/local/lib/python3.6/dist-packages (from spacy<2.0.0,>=1.7.0->en-core-web-sm==1.2.0)\n",
            "Requirement already satisfied: numpy>=1.7 in /usr/local/lib/python3.6/dist-packages (from spacy<2.0.0,>=1.7.0->en-core-web-sm==1.2.0)\n",
            "Requirement already satisfied: murmurhash<0.27,>=0.26 in /usr/local/lib/python3.6/dist-packages (from spacy<2.0.0,>=1.7.0->en-core-web-sm==1.2.0)\n",
            "Requirement already satisfied: pip<10.0.0,>=9.0.0 in /usr/local/lib/python3.6/dist-packages (from spacy<2.0.0,>=1.7.0->en-core-web-sm==1.2.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy<2.0.0,>=1.7.0->en-core-web-sm==1.2.0)\n",
            "Requirement already satisfied: cymem<1.32,>=1.30 in /usr/local/lib/python3.6/dist-packages (from spacy<2.0.0,>=1.7.0->en-core-web-sm==1.2.0)\n",
            "Requirement already satisfied: dill<0.3,>=0.2 in /usr/local/lib/python3.6/dist-packages (from spacy<2.0.0,>=1.7.0->en-core-web-sm==1.2.0)\n",
            "Requirement already satisfied: plac<1.0.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy<2.0.0,>=1.7.0->en-core-web-sm==1.2.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.6/dist-packages (from thinc<6.6.0,>=6.5.0->spacy<2.0.0,>=1.7.0->en-core-web-sm==1.2.0)\n",
            "Requirement already satisfied: cytoolz<0.9,>=0.8 in /usr/local/lib/python3.6/dist-packages (from thinc<6.6.0,>=6.5.0->spacy<2.0.0,>=1.7.0->en-core-web-sm==1.2.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.6/dist-packages (from thinc<6.6.0,>=6.5.0->spacy<2.0.0,>=1.7.0->en-core-web-sm==1.2.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.10.0 in /usr/local/lib/python3.6/dist-packages (from thinc<6.6.0,>=6.5.0->spacy<2.0.0,>=1.7.0->en-core-web-sm==1.2.0)\n",
            "Requirement already satisfied: html5lib in /usr/local/lib/python3.6/dist-packages (from ftfy<5.0.0,>=4.4.2->spacy<2.0.0,>=1.7.0->en-core-web-sm==1.2.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from ftfy<5.0.0,>=4.4.2->spacy<2.0.0,>=1.7.0->en-core-web-sm==1.2.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy<2.0.0,>=1.7.0->en-core-web-sm==1.2.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy<2.0.0,>=1.7.0->en-core-web-sm==1.2.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy<2.0.0,>=1.7.0->en-core-web-sm==1.2.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy<2.0.0,>=1.7.0->en-core-web-sm==1.2.0)\n",
            "Requirement already satisfied: toolz>=0.8.0 in /usr/local/lib/python3.6/dist-packages (from cytoolz<0.9,>=0.8->thinc<6.6.0,>=6.5.0->spacy<2.0.0,>=1.7.0->en-core-web-sm==1.2.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.6/dist-packages (from html5lib->ftfy<5.0.0,>=4.4.2->spacy<2.0.0,>=1.7.0->en-core-web-sm==1.2.0)\n",
            "Installing collected packages: en-core-web-sm\n",
            "  Found existing installation: en-core-web-sm 2.2.5\n",
            "    Uninstalling en-core-web-sm-2.2.5:\n",
            "      Successfully uninstalled en-core-web-sm-2.2.5\n",
            "  Running setup.py install for en-core-web-sm ... \u001b[?25ldone\n",
            "\u001b[?25hSuccessfully installed en-core-web-sm-1.2.0\n",
            "\n",
            "\u001b[93m    Linking successful\u001b[0m\n",
            "\n",
            "    /usr/local/lib/python3.6/dist-packages/en_core_web_sm/en_core_web_sm-1.2.0\n",
            "    --> /usr/local/lib/python3.6/dist-packages/spacy/data/en\n",
            "\n",
            "    You can now load the model via spacy.load('en').\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ob7qL6kUVjbu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Importar todos os pacotes de uma só vez para evitar duplicados ao longo do notebook.\n",
        "import datetime\n",
        "import errant\n",
        "import gzip\n",
        "import json\n",
        "import nvidia_smi\n",
        "import os\n",
        "import pickle\n",
        "import psutil\n",
        "import pytorch_lightning as pl\n",
        "import random\n",
        "import spacy\n",
        "import sys\n",
        "import tarfile\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from argparse import Namespace\n",
        "from google.colab import drive\n",
        "\n",
        "from pytorch_lightning.callbacks import ModelCheckpoint\n",
        "from pytorch_lightning.loggers import TensorBoardLogger\n",
        "from pytorch_lightning import Trainer\n",
        "\n",
        "from transformers import T5ForConditionalGeneration, T5Model\n",
        "from transformers import T5Tokenizer\n",
        "from transformers import BertTokenizer, BertForMaskedLM\n",
        "from torch.optim import Adam\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import Dataset\n",
        "from torch import nn\n",
        "\n",
        "from typing import Dict\n",
        "from typing import List\n",
        "from typing import Tuple\n",
        "\n",
        "# Portuguese corpus for vocab\n",
        "import nltk\n",
        "from nltk.corpus import floresta\n",
        "nltk.download('floresta')\n",
        "floresta_vocab = floresta.words()\n",
        "\n",
        "# Leard decoding solution\n",
        "import html\n",
        "import unicodedata\n",
        "\n",
        "nlp = spacy.load(\"en\")\n",
        "annotator = errant.load(\"en\", nlp)\n",
        "\n",
        "\n",
        "def hardware_stats():\n",
        "    \"\"\"\n",
        "    Returns a dict containing some hardware related stats\n",
        "    \"\"\"\n",
        "    res = nvidia_smi.nvmlDeviceGetUtilizationRates(handle)\n",
        "    return {\"cpu\": f\"{str(psutil.cpu_percent())}%\",\n",
        "            \"mem\": f\"{str(psutil.virtual_memory().percent)}%\",\n",
        "            \"gpu\": f\"{str(res.gpu)}%\",\n",
        "            \"gpu_mem\": f\"{str(res.memory)}%\"}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tm8H4Rfwm2gE",
        "colab_type": "text"
      },
      "source": [
        "## Define random seeds\n",
        "\n",
        "Importante: Fix seeds so we can replicate results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bJlZDb1VY29r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import random\n",
        "\n",
        "seed = 0\n",
        "random.seed(seed)\n",
        "torch.random.manual_seed(seed)\n",
        "torch.cuda.manual_seed(seed)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fs5GhqmMTNNS",
        "colab_type": "text"
      },
      "source": [
        "DICA para modelos reais: Um modelo otimizado deve manter o uso de GPU próximo a 100% durante o treino.\n",
        "Vamos utilizar a bilioteca abaixo para monitorar isso. Note que no modelo simples utilizado aqui o uso não vai chegar a 100%."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UGNTZKHrTM6l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(f\"Pytorch Lightning Version: {pl.__version__}\")\n",
        "nvidia_smi.nvmlInit()\n",
        "handle = nvidia_smi.nvmlDeviceGetHandleByIndex(0)\n",
        "print(f\"Device name: {nvidia_smi.nvmlDeviceGetName(handle)}\")\n",
        "\n",
        "def gpu_usage():\n",
        "    global handle\n",
        "    return f\"{str(nvidia_smi.nvmlDeviceGetUtilizationRates(handle).gpu)}%\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ETfkvMGl4JA1",
        "colab_type": "text"
      },
      "source": [
        "## Mapeia Google Drive\n",
        "\n",
        "Iremos salvar os checkpoints (pesos do modelo) no google drive, para que possamos continuar o treino de onde paramos."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "hvGjweSKfbA2",
        "colab": {}
      },
      "source": [
        "drive.mount(\"/content/drive\")\n",
        "base_path = \"/content/drive/My Drive/PF-Correcao/bea2019st\"\n",
        "base_path = \"/content/bea2019st\"\n",
        "os.environ[\"BASE_PATH\"] = base_path"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ck76D7JWBI5E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ime vocab https://www.ime.usp.br/~pf/dicios/index.html\n",
        "ime_words_path = \"/content/drive/My Drive/PF-Correcao/br-utf8.txt\"\n",
        "with open(ime_words_path) as file: # Use file to refer to the file object\n",
        "   ime_vocab = file.read().splitlines()\n",
        "\n",
        "print(ime_vocab[:10])\n",
        "print(len(ime_vocab))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BMhp7hvN9mS6",
        "colab_type": "text"
      },
      "source": [
        "## ERRANT Scorer\n",
        "\n",
        "Comando para avaliação que compara um arquivo M2 \"hipótese\" contra um arquivo M2 \"referência\".<br>\n",
        "\n",
        "### **Example**\n",
        "**Original**: This are gramamtical sentence .<br>\n",
        "**Corrected**: This is a grammatical sentence .<br>\n",
        "**Output M2**:<br>\n",
        "S This are gramamtical sentence .<br>\n",
        "A 1 2|||R:VERB:SVA|||is|||REQUIRED|||-NONE-|||0<br>\n",
        "A 2 2|||M:DET|||a|||REQUIRED|||-NONE-|||0<br>\n",
        "A 2 3|||R:SPELL|||grammatical|||REQUIRED|||-NONE-|||0<br>\n",
        "A -1 -1|||noop|||-NONE-|||REQUIRED|||-NONE-|||1<br>\n",
        "\n",
        "In M2 format, a line preceded by S denotes an original sentence while a line preceded by A indicates an edit annotation. Each edit line consists of the start and end token offset of the edit, the error type, and the tokenized correction string. The next two fields are included for historical reasons (see the CoNLL-2014 shared task) while the last field is the annotator id."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aWPWngE89ss_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%writefile ref.m2\n",
        "S This are gramamtical sentence .\n",
        "A 1 2|||R:VERB:SVA|||is|||REQUIRED|||-NONE-|||0\n",
        "A 2 2|||M:DET|||a|||REQUIRED|||-NONE-|||0\n",
        "A 2 3|||R:SPELL|||grammatical|||REQUIRED|||-NONE-|||0\n",
        "A -1 -1|||noop|||-NONE-|||REQUIRED|||-NONE-|||1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wvnqc0xk9svq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%writefile hyp.m2\n",
        "S This are gramamtical sentence .\n",
        "A 1 2|||R:VERB:SVA|||is|||REQUIRED|||-NONE-|||0\n",
        "A 2 2|||M:DET|||a|||REQUIRED|||-NONE-|||0\n",
        "A 2 3|||R:SPELL|||grammatical|||REQUIRED|||-NONE-|||0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HJSrllNQb9kw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "original = 'This are gramamtical sentence .'\n",
        "correct = 'This is a grammatical sentence .'\n",
        "annotator = errant.load('en')\n",
        "orig = annotator.parse(original)\n",
        "cor = annotator.parse(correct)\n",
        "edits = annotator.annotate(orig, cor, lev=False, merging='rules')\n",
        "\n",
        "with open('hyp.m2', 'w') as f:\n",
        "    f.write(f'S {original}\\n')\n",
        "    for edit in edits:\n",
        "        f.write(f'{edit.to_m2()}\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a6KxhquG8U44",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cat hyp.m2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qj0qyFWK9pI3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!errant_compare -hyp hyp.m2 -ref ref.m2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dJl-bFrA5k6h",
        "colab_type": "text"
      },
      "source": [
        "# predição"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YmxoZCZoIq6N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Input text\n",
        "text = '[CLS] Eu gosto [MASK] brigadeiro [SEP]'\n",
        "tokenizer = BertTokenizer.from_pretrained(\"neuralmind/bert-base-portuguese-cased\")\n",
        "model = BertForMaskedLM.from_pretrained(\"neuralmind/bert-base-portuguese-cased\")\n",
        "model.eval()\n",
        "\n",
        "# Print the original sentence.\n",
        "print(' Original: ', text)\n",
        "# Print the sentence split into tokens.\n",
        "print('Tokenized: ', tokenizer.tokenize(text))\n",
        "# Print the sentence mapped to token ids.\n",
        "print('Token IDs: ', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(text)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "22l_FHOEmJjM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "text = 'Eu [MASK] de brigadeiro.'\n",
        "encoded = tokenizer.encode_plus(text, add_special_tokens=True, return_tensors='pt')\n",
        "input_ids = encoded['input_ids']\n",
        "\n",
        "# Generating 20 sequences with maximum length set to 5\n",
        "outputs = model.generate(input_ids=input_ids, num_beams=200,\n",
        "                         num_return_sequences=20, max_length=5)\n",
        "results = list(map(tokenizer.decode, outputs))\n",
        "\n",
        "print(input_ids)\n",
        "print(outputs)\n",
        "results"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hIqrUhaP2Hi9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "text = 'Eu [MASK] de brigadeiro.'\n",
        "encoded = tokenizer.encode_plus(text, add_special_tokens=True, return_tensors='pt')\n",
        "input_ids = encoded['input_ids']\n",
        "\n",
        "# Generating 20 sequences with maximum length set to 5\n",
        "outputs = model.generate(input_ids=input_ids, num_beams=200,\n",
        "                         num_return_sequences=20, max_length=5)\n",
        "\n",
        "mask_idx = text.index('[MASK]')\n",
        "_result_prefix = text[:mask_idx]\n",
        "_result_suffix = text[mask_idx+6:] \n",
        "\n",
        "def _filter(output, end_token='[SEP]'):\n",
        "    # The first token is <unk> (inidex at 0) and the second token is <extra_id_0> (indexed at 32099)\n",
        "    _txt = tokenizer.decode(output[1:], skip_special_tokens=False, clean_up_tokenization_spaces=False)\n",
        "    print(_txt)\n",
        "    if end_token in _txt:\n",
        "        _end_token_index = _txt.index(end_token)\n",
        "        return _result_prefix + _txt[:_end_token_index] + _result_suffix\n",
        "    else:\n",
        "        return _result_prefix + _txt + _result_suffix\n",
        "\n",
        "results = list(map(_filter, outputs))\n",
        "\n",
        "print(input_ids)\n",
        "print(outputs)\n",
        "results"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "doGfYSmeyp2l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "text = \"Eu [MASK] de brigadeiro.\"\n",
        "input_ids = tokenizer.encode(text, return_tensors='pt')\n",
        "mask_pos = input_ids.tolist()[0].index(103)\n",
        "k=10\n",
        "outputs = model(input_ids=input_ids)\n",
        "_, outputs = torch.topk(outputs[0], k, sorted=True)\n",
        "results = list(map(tokenizer.decode, outputs.squeeze()))\n",
        "print(input_ids)\n",
        "print(outputs.squeeze())\n",
        "results"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rsbsTMWZ57im",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "text = \"Eu amo João [MASK].\"\n",
        "input_ids = tokenizer.encode(text, return_tensors='pt')\n",
        "mask_pos = input_ids.tolist()[0].index(103)\n",
        "k=10\n",
        "outputs = model(input_ids=input_ids)\n",
        "_, outputs = torch.topk(outputs[0], k, sorted=True)\n",
        "results = list(map(tokenizer.decode, outputs.squeeze()))\n",
        "print(input_ids)\n",
        "print(outputs.squeeze())\n",
        "results[mask_pos]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XM4nKd-owcW9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "seq = \"Eu gosto de brigadeiro.\"\n",
        "k = 50\n",
        "seq_tokens_id = tokenizer.encode(seq)\n",
        "for idx in range(len(seq_tokens_id)):\n",
        "  inputs_id = seq_tokens_id.copy()\n",
        "  if inputs_id[idx] == tokenizer.cls_token_id or inputs_id[idx] == tokenizer.sep_token_id:\n",
        "    continue\n",
        "  inputs_id[idx] = tokenizer.mask_token_id \n",
        "  print(tokenizer.decode(inputs_id))\n",
        "  inputs_id = torch.LongTensor(input_ids)\n",
        "  outputs = model(input_ids=input_ids)\n",
        "  _, outputs = torch.topk(outputs[0], k, sorted=True)\n",
        "  results = list(map(tokenizer.decode, outputs.squeeze()))[idx]\n",
        "  print(results)\n",
        "  print()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VAF8n2ldrYYR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "seq = \"Eu estou gostando de brigadeiro.\"\n",
        "seq_splitted = re.findall(r\"[\\w']+|[.,!?;]\", seq)\n",
        "k = 5\n",
        "inputs_id = tokenizer.encode(seq_splitted, return_tensors='pt')\n",
        "\n",
        "for idx in range(inputs_id.shape[1]):\n",
        "  input_seq = inputs_id.clone()\n",
        "  input_seq[0][idx] = tokenizer.mask_token_id\n",
        "  #input_seq = ' '.join(input_seq)\n",
        "  print(input_seq)\n",
        "  \n",
        "  print(tokenizer.decode(inputs_id[0]))\n",
        "  #for token in inputs_id[0].tolist():\n",
        "  #  print(token, tokenizer.decode(token))\n",
        "  outputs = model(input_ids=input_ids)\n",
        "  _, outputs = torch.topk(outputs[0], k, sorted=True)\n",
        "  results = list(map(tokenizer.decode, outputs.squeeze()))\n",
        "  print(len(results), len(inputs_id[0]))\n",
        "  print(idx, results[idx])\n",
        "  print()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "4SEOCAtdGOFi",
        "colab": {}
      },
      "source": [
        "import re\n",
        "seq = \"Eu estou gostando de brigadeiro.\"\n",
        "seq_splitted = re.findall(r\"[\\w']+|[.,!?;]\", seq)\n",
        "k = 10\n",
        "inputs_id = tokenizer.encode(seq_splitted, return_tensors='pt')\n",
        "inputs_id = inputs_id.repeat(len(seq_splitted), 1)\n",
        "mask_pos = []\n",
        "for idx, tensor in enumerate(inputs_id):\n",
        "  tensor[idx+1] = tokenizer.mask_token_id\n",
        "  mask_pos.append(idx+1)\n",
        "print(inputs_id)\n",
        "model.eval()\n",
        "outputs = model(input_ids=inputs_id)\n",
        "_, outputs = torch.topk(outputs[0], k, sorted=True)\n",
        "\n",
        "for idx in mask_pos:\n",
        "  print(inputs_id[idx-1])\n",
        "  results = list(map(tokenizer.decode, outputs[idx-1].squeeze()))[idx]\n",
        "  print(results)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "up3JASe5J_lZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "text = \"Eu gosto de brigadeiro.\"\n",
        "splitted = text.split()\n",
        "for idx, word in enumerate(splitted):\n",
        "  splitted[idx] = '[MASK]'\n",
        "  new_text = ' '.join(splitted)\n",
        "  print(new_text)\n",
        "  input_ids = tokenizer.encode(new_text, return_tensors='pt')\n",
        "  mask_pos = input_ids.tolist()[0].index(103)\n",
        "  k=10\n",
        "  outputs = model(input_ids=input_ids)\n",
        "  _, outputs = torch.topk(outputs[0], k, sorted=True)\n",
        "  results = list(map(tokenizer.decode, outputs.squeeze()))\n",
        "  print(results[mask_pos])\n",
        "  splitted[idx] = word"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r7hrZtF6QJ3E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "text = \"Eu gsto de brigadeiro.\"\n",
        "splitted = text.split()\n",
        "for idx, word in enumerate(splitted):\n",
        "  splitted[idx] = '[MASK]'\n",
        "  new_text = ' '.join(splitted)\n",
        "  print(new_text)\n",
        "  input_ids = tokenizer.encode(new_text, return_tensors='pt')\n",
        "  mask_pos = input_ids.tolist()[0].index(103)\n",
        "  k=10\n",
        "  outputs = model(input_ids=input_ids)\n",
        "  _, outputs = torch.topk(outputs[0], k, sorted=True)\n",
        "  results = list(map(tokenizer.decode, outputs.squeeze()))\n",
        "  print(results[mask_pos])\n",
        "  splitted[idx] = word"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EdZLtxv6Mvl4",
        "colab_type": "text"
      },
      "source": [
        "# comparação"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jwITaxaPSnEK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip3 install fuzzywuzzy[speedup] "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U-PA6uWYMxYP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from fuzzywuzzy import fuzz, process, StringMatcher\n",
        "fuzz.SequenceMatcher = StringMatcher.StringMatcher\n",
        "#import difflib\n",
        "#fuzz.SequenceMatcher = difflib.SequenceMatcher\n",
        "\n",
        "# Correção\n",
        "word = 'gsto'\n",
        "pred = ['gosto', 'gostava', 'amo', 'sou', 'preciso', 'precisava', 'vou', 'iria', 'morro', 'ia']\n",
        "\n",
        "metric_value_default = [fuzz.ratio(word, pred_word) for pred_word in pred]\n",
        "\n",
        "print(metric_value_default)\n",
        "print(process.extract(word, pred, limit=4))\n",
        "print(process.extractOne(word, pred))\n",
        "\n",
        "# Sem correção\n",
        "word = 'Eu'\n",
        "pred = ['Mas', 'E', 'Já', 'Não', 'Agora', 'Hoje', 'Ainda', 'Então', '[UNK]']\n",
        "\n",
        "metric_value_default = [fuzz.ratio(word, pred_word) for pred_word in pred]\n",
        "\n",
        "print(metric_value_default)\n",
        "print(process.extract(word, pred, limit=4))\n",
        "print(process.extractOne(word, pred))\n",
        "\n",
        "# correção\n",
        "word = 'a'\n",
        "pred = ['à', 'as', 'ai', 'á', 'ã', 'ia']\n",
        "\n",
        "metric_value_default = [fuzz.ratio(word, pred_word) for pred_word in pred]\n",
        "\n",
        "print(metric_value_default)\n",
        "\n",
        "print(process.extract(word, pred, limit=4))\n",
        "print(process.extractOne(word, pred))\n",
        "print('scorer:', process.extractOne(word, pred, scorer=fuzz.ratio))\n",
        "\n",
        "\n",
        "print(pred[metric_value_default.index(max(metric_value_default))])\n",
        "matches_id = [i for i,x in enumerate(metric_value_default) if x==max(metric_value_default)]\n",
        "print(matches_id)\n",
        "matches = [pred[x] for x in matches_id]\n",
        "print(matches)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mr-H-Kx05oA7",
        "colab_type": "text"
      },
      "source": [
        "# modelo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XwPftG--5qNt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class bert_pt_corrector(nn.Module):\n",
        "  '''\n",
        "  Bert inference model\n",
        "  '''\n",
        "  def __init__(self, k_top_predictions=10, levenshtein_threshold=85,\n",
        "               vocab=ime_vocab):\n",
        "    super().__init__()\n",
        "\n",
        "    # Inicializa modelo e tokenizador\n",
        "    self.tokenizer = BertTokenizer.from_pretrained(\"neuralmind/bert-base-portuguese-cased\")\n",
        "    self.bert = BertForMaskedLM.from_pretrained(\"neuralmind/bert-base-portuguese-cased\")\n",
        "    for param in self.bert.bert.parameters():\n",
        "      param.requires_grad = False\n",
        "\n",
        "    # Set number of predictions to look\n",
        "    self.k = k_top_predictions\n",
        "\n",
        "    # Set threshold for Levenshtein comparison\n",
        "    self.threshold = levenshtein_threshold\n",
        "\n",
        "    self.vocab = vocab\n",
        "    \n",
        "    #self.punctuation = ['.', ',', ':', ';', '?', '!']\n",
        "  \n",
        "  def forward(self, sequence):\n",
        "    suggestion = []\n",
        "    splitted = sequence.split()\n",
        "    #print(splitted)\n",
        "    for idx, word in enumerate(splitted):\n",
        "      #print(word)\n",
        "      n_encoder = len(self.tokenizer.encode(word))\n",
        "      #print(n_encoder)\n",
        "      new_value = n_encoder * ['[MASK]']\n",
        "      new_value = ' '.join(new_value)\n",
        "      #print(new_value)\n",
        "      splitted[idx] = new_value\n",
        "      new_text = ' '.join(splitted)\n",
        "      print(new_text)\n",
        "      #print(new_text)\n",
        "      #print(new_text)\n",
        "      input_ids = self.tokenizer.encode(new_text, return_tensors='pt')\n",
        "      mask_pos_init = input_ids.tolist()[0].index(103)\n",
        "      mask_pos_end = input_ids.tolist()[0][::-1].index(103)\n",
        "      splitted[idx] = word\n",
        "      print(input_ids)\n",
        "\n",
        "      outputs = self.bert(input_ids=input_ids)\n",
        "      print('bert_out', len(outputs), type(outputs), outputs)\n",
        "\n",
        "      _, outputs = torch.topk(outputs[0], self.k, sorted=True)\n",
        "      print('topk_out', outputs.shape, type(outputs))\n",
        "\n",
        "      results = list(map(self.tokenizer.decode, outputs.squeeze()))\n",
        "      #print(mask_pos, results[mask_pos])\n",
        "      print('rst_len', len(results))\n",
        "      for idx, rst in enumerate(results):\n",
        "        print(input_ids[0][idx])\n",
        "        print(rst)\n",
        "      prediction = results[mask_pos_init].split()\n",
        "      #prediction = [word for result in results for word in result.split()]\n",
        "      #prediction = list(set(prediction))\n",
        "      #print(prediction)\n",
        "\n",
        "      only_word = re.sub(r'[.,!?;]', '', word)\n",
        "      if only_word in prediction:\n",
        "        suggestion.append(word)\n",
        "        #print('Only copy:', word)\n",
        "      else:\n",
        "        near_word = self.get_near_word(only_word, prediction)\n",
        "        if near_word is None:\n",
        "          suggestion.append(word)\n",
        "          #print('No suggestion, word copied:', word)\n",
        "        else:\n",
        "          word_split = re.findall(r\"[\\w']+|[.,!?;]\", word)\n",
        "          word_split[word_split==only_word] = near_word\n",
        "          word_suggestion = ''.join(word_split)\n",
        "          suggestion.append(word_suggestion)\n",
        "    \n",
        "    suggestion = ' '.join(suggestion)\n",
        "    return suggestion\n",
        "\n",
        "  def get_near_word(self, word, prediction):\n",
        "    #prediction = list(set(prediction).intersection(self.vocab)) # muito lento\n",
        "    #comparison_values = [fuzz.ratio(word, pred_word) for pred_word in prediction]\n",
        "    #max_value = max(comparison_values)\n",
        "    #print('W', word)\n",
        "    #print('P', prediction)\n",
        "    #print('C', comparison_values)\n",
        "    near_word, max_value = process.extractOne(word, prediction)\n",
        "    #print('S', near_word, max_value)\n",
        "    if max_value < self.threshold:\n",
        "      return None\n",
        "\n",
        "    #matches_id = [i for i,x in enumerate(comparison_values)\n",
        "    #              if x==max(comparison_values)]\n",
        "    #near_words = [prediction[x] for x in matches_id]\n",
        "    #print('S', near_words[0], max_value)\n",
        "    return near_word#near_words[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mp3nSNaZbhRK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Testing model\n",
        "\n",
        "sequence = \"Capitu, apesar daqueles olhos que o diabo lhe deu... \\\n",
        "Você já reparou nos olhos dela? São assim de cigana oblíqua e dissimulada.\"\n",
        "\n",
        "sequence = \"Capitu, brigadeiro Pota.\"\n",
        "\n",
        "spell_checker = bert_pt_corrector()\n",
        "spell_checker.eval()\n",
        "spell_checker(sequence)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "owxGNQO-dDQg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Testing model\n",
        "\n",
        "sequence = \"Capitu, apesar daqueles olhos q o diabo lhe deu... \\\n",
        "Vc ja reparou nos olhos dela? Sao assim de cigana obliqua e dissimulada.\"\n",
        "\n",
        "spell_checker = bert_pt_corrector(k_top_predictions=50)\n",
        "spell_checker(sequence)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VxhdxsHHrMlU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Testing model\n",
        "\n",
        "sequence = \"Capitu, apesar daqueles olhos q o diabo lhe deu... \\\n",
        "Vc ja reparou nos olhos dela? Sao assim de cigana obliqua e dissimulada.\"\n",
        "\n",
        "spell_checker = bert_pt_corrector(k_top_predictions=500, levenshtein_threshold=50)\n",
        "spell_checker(sequence)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ramFux0-yHf3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Testing model\n",
        "\n",
        "sequence = \"Capitu, apesar daqueles olhos q o diabo lhe deu... \\\n",
        "Vc ja reparou nos olhos dela? Sao assim de cigana obliqua e dissimulada.\"\n",
        "\n",
        "spell_checker = bert_pt_corrector(k_top_predictions=1000, levenshtein_threshold=50)\n",
        "spell_checker(sequence)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UXI0MHWU74Cz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'Xq' in ime_vocab#['geral', 'iguais', 'chegou', 'gostou', 'poderes', 'qualidades', 'chamado', 'mal', 'logo', 'desconte', 'doou', 'forneceu', 'viu', 'óculos', 'assistiu', 'trinta', 'ato', 'horror', 'justamente', 'lançados', 'cio', 'nascidos', 'lixo', 'da', 'presos', 'tempo', 'nele', 'conversa', 'formou', 'marcou', 'Cristo', 'lentes', 'companheira', 'cal', 'buracos', 'inter', 'dourado', 'conta', 'devido', 'incluir', 'papel', 'citou', 'animais', 'criou', 'figuras', 'confirmou', 'serem', 'centros', 'metade', 'sentimentos', 'sintomas', 'firme', 'simples', 'estava', 'traços', 'sistema', 'imagem', 'partidos', 'qual', 'coro', 'metal', 'leitores', 'negra', 'vale', 'politicamente', 'cinzas', 'luz', 'voltados', 'repara', 'tatu', 'por', 'dei', 'São', 'time', 'exemplos', 'firma', 'imediata', 'o', 'peças', 'este', 'verdade', 'sorte', 'descendentes', 'existe', 'nas', 'fundos', 'para', 'com', 'conhecer', 'tele', 'falsa', 'continua', 'vendo', 'nessa', 'pela', 'membros', 'leitor', 'apareceu', 'prometeu', 'pôde', 'provavelmente', 'vil', 'aquelas', 'outrora', 'mostra', 'pesados', 'tais', 'coisas', 'conselhos', 'montes', 'mulher', 'espirituais', 'pastor', 'reserva', 'si', 'assistir', 'me', 'desconhecidos', 'fato', 'sinais', 'chance', 'sombra', 'porte', 'primeiros', 'muita', 'tinha', 'baixos', 'estiver', 'foram', 'diferentemente', 'fechados', 'lado', 'melhores', 'santos', 'aos', 'via', 'lados', 'tirou', 'modos', 'suas', 'cães', 'possui', 'li', 'carro', 'propósitos', 'ainda', 'desses', 'conto', 'lógica', 'selvagens', 'mecânica', 'flor', 'cinza', 'feitas', 'frios', 'sue', 'conhece', 'reais', 'exames', 'muito', 'comer', 'dessas', 'formas', 'através', 'são', 'perdida', 'dois', 'sobre', 'foco', 'partes', 'treze', 'tu', 'das', 'abertos', 'caminho', 'determinou', 'encontrou', 'verdadeiros', 'dourada', 'recebe', 'estranho', 'colocou', 'propósito', 'fruto', 'lei', 'poucos', 'fria', 'dado', 'ajudou', 'judeus', 'perguntou', 'cu', 'recebeu', 'rosa', 'capas', 'femininos', 'morta', 'compostos', 'pôs', 'desistiu', 'moda', 'ata', 'assassinos', 'subiu', 'oferecer', 'simplesmente', 'própria', 'está', 'bem', 'internos', 'mortais', 'movimentos', 'sou', 'porque', 'homem', 'pediram', 'levou', 'infantis', 'raça', 'garotos', 'estar', 'rendeu', 'dada', 'Sul', 'claro', 'lido', 'entre', 'naquele', 'vis', 'pernas', 'restos', 'diante', 'tudo', 'mata', 'hem', 'nesse', 'ao', 'separou', 'orai', 'antigos', 'anal', 'demo', 'vistos', 'comentários', 'feitos', 'sei', 'meus', 'aqui', 'esforços', 'bar', 'diz', 'for', 'general', 'negou', 'duma', 'quadros', 'revista', 'semelhança', 'ser', 'apesar', 'apresentou', 'conselheiros', 'demônio', 'havia', 'escritos', 'mel', 'conhecidos', 'onde', 'completamente', 'reduzida', 'bom', 'deram', 'deste', 'falta', 'meia', 'nossa', 'quem', 'foca', 'poderosos', 'fãs', 'Maria', 'matou', 'gigantes', 'nenhum', 'nos', 'vive', 'acha', 'bel', 'entregou', 'cura', 'comida', 'donos', 'padrões', 'proporcionar', 'Jorge', 'pá', 'magos', 'tira', 'acaba', 'ma', 'poderia', 'fim', 'mundo', 'do', 'aquele', 'olhos', 'pediu', 'ganhos', 'estudos', 'eva', 'feminina', 'jeito', 'lo', 'local', 'comprou', 'embora', 'normalmente', 'profissional', 'neta', 'concedido', 'certos', 'fotos', 'sequer', 'usou', 'últimos', 'cama', 'sente', 'daqueles', 'eram', 'intelectual', 'pode', 'oferece', 'certamente', 'assuntos', 'Daniel', 'tratando', 'cuidados', 'eu', 'mãos', 'cedeu', 'ris', 'sabia', 'sendo', 'dias', 'pois', 'pares', 'africana', 'jantar', 'cheio', 'originalmente', 'junta', 'tratados', 'ovos', 'gala', 'deles', 'pesa', 'hoje', 'ares', 'isso', 'Eva', 'descont', 'em', 'especial', 'pura', 'ruim', 'conceder', 'aquilo', 'papa', 'maca', 'fora', 'nosso', 'parece', 'maus', 'amor', 'hábitos', 'deixou', 'forma', 'se', 'deu', 'céu', 'voltou', 'ele', 'igualmente', 'atos', 'orientada', 'Marta', 'pegar', 'sai', 'sempre', 'três', 'ultra', 'bastante', 'externos', 'mostrou', 'próprios', 'Grande', 'milhões', 'nem', 'salvo', 'virtude', 'mesma', 'esteve', 'perdão', 'dano', 'longe', 'desse', 'enviou', 'pro', 'chama', 'desde', 'comandante', 'cai', 'acerca', 'brilhantes', 'hum', 'auto', 'lava', 'pele', 'rebeldes', 'pelo', 'grandes', 'acima', 'dentro', 'funda', 'ora', 'aí', 'pensar', 'atriz', 'primeira', 'bons', 'contida', 'trabalhou', 'indica', 'falou', 'padre', 'raio', 'estilo', 'sexuais', 'foto', 'dos', 'precisa', 'anjo', 'queria', 'pensamentos', 'os', 'ate', 'ricos', 'tia', 'ti', 'tanta', 'florestas', 'prestou', 'citados', 'nobres', 'ganhou', 'detalhes', 'aparência', 'cri', 'mo', 'informa', 'novos', 'campos', 'há', 'desenhos', 'também', 'pau', 'achou', 'fama', 'salva', 'agente', 'como', 'come', 'conversas', 'respectivos', 'reuniu', 'histórias', 'nós', 'principalmente', 'dessa', 'cabelos', 'alvos', 'colocar', 'lote', 'sejamos', 'tantos', 'aplicou', 'força', 'grãos', 'divino', 'pena', 'tão', 'humanos', 'lhes', 'dá', 'objetiva', 'casos', 'chega', 'per', 'criados', 'estes', 'poder', 'gosta', 'falar', 'biológica', 'ficou', 'enfim', 'mar', 'foi', 'lá', 'naquela', 'as', 'EUA', 'independente', 'outra', 'relato', 'apenas', 'toda', 'uns', 'cortes', 'deputados', 'declarada', 'compro', 'escolheu', 'essas', 'namorada', 'agora', 'sub', 'nada', 'dirigiu', 'dedicada', 'deve', 'mais', 'benefícios', 'pra', 'ego', 'fracos', 'mas', 'tal', 'arredores', 'coma', 'sua', 'contra', 'dela', 'falava', 'bit', 'aqueles', 'comentar', 'pessoas', 'lembra', 'repor', 'pobre', 'ater', 'emitiu', 'pecado', 'sido', 'tinham', 'aonde', 'costume', 'diferente', 'passados', 'gosto', 'mentais', 'der', 'olho', 'delas', 'fuma', 'aba', 'concedeu', 'perfeita', 'umas', 'visões', 'palha', 'quanto', 'nela', 'quase', 'remanescente', 'investi', 'super', 'vários', 'despeito', 'depois', 'já', 'antes', 'cano', 'davam', 'pergunta', 'gente', 'você', 'brasileiros', 'problemas', 'norte', 'veio', 'eles', 'médicos', 'quando', 'azuis', 'ver', 'alta', 'pedir', 'visto', 'cuida', 'observou', 'tiveram', 'especiais', 'ali', 'amigos', 'fica', 'então', 'romana', 'responsável', 'esposa', 'só', 'destes', 'maneira', 'tons', 'visitantes', 'continuando', 'debaixo', 'acusado', 'casa', 'privilégios', 'campanha', 'tipos', 'violentos', 'beleza', 'tirar', 'critica', 'a', 'pobres', 'vestido', 'impostos', 'age', 'não', 'pais', 'seu', 'outros', 'presentes', 'carros', 'bonita', 'detectar', 'nova', 'dou', 'acabou', 'locais', 'um', 'diferentes', 'deixar', 'antiga', 'sol', 'algo', 'baixa', 'qualquer', 'notar', 'lua', 'voltando', 'continuou', 'era', 'doentes', 'vermelho', 'vete', 'natureza', 'acredita', 'realmente', 'frases', 'cheia', 'rosto', 'poderosa', 'mestre', 'cor', 'fie', 'hora', 'faz', 'padrão', 'cristã', 'determinados', 'boca', 'disco', 'ler', 'filmes', 'entende', 'moça', 'amiga', 'questiona', 'semana', 'ano', 'dia', 'tomou', 'vista', 'típico', 'exatamente', 'parou', 'manda', 'muitas', 'sapatos', 'preparou', 'vento', 'nenhuma', 'amarela', 'cria', 'índices', 'termos', 'tomar', 'hospital', 'naturais', 'viram', 'aula', 'vide', 'companheiros', 'ganha', 'Fátima', 'dava', 'rei', 'espanhola', 'algum', 'mau', 'políticos', 'esquerda', 'destina', 'longos', 'anos', 'coisa', 'vida', 'espírito', 'corre', 'livros', 'essa', 'mandou', 'sabendo', 'mim', 'roupas', 'piso', 'Norte', 'personagem', 'pouca', 'roma', 'toneladas', 'falando', 'alguém', 'disto', 'inferno', 'invés', 'minha', 'negros', 'saiu', 'aspectos', 'ter', 'parecido', 'santo', 'sim', 'vir', 'mono', 'apresentação', 'Mar', 'conseguiu', 'objetos', 'estranha', 'esse', 'jovem', 'vem', 'quentes', 'retratos', 'som', 'sonhos', 'acusados', 'bando', 'fez', 'exceção', 'feita', 'deixa', 'grande', 'olhar', 'outro', 'pedaços', 'pesco', 'pontos', 'mudou', 'povo', 'quer', 'legal', 'meu', 'reclama', 'quis', 'no', 'tê', 'pés', 'religiosos', 'ama', 'acesso', 'nó', 'assim', 'sofre', 'extremamente', 'serviu', 'longo', 'anda', 'pequenos', 'manchas', 'mos', 'ofereceu', 'mor', 'disse', 'par', 'efeitos', 'cara', 'demais', 'daria', 'jamais', 'azul', 'esta', 'chamados', 'tomaram', 'visão', 'familiares', 'holandeses', 'mira', 'capaz', 'secos', 'simpatia', 'destino', 'dar', 'esses', 'te', 'erros', 'verde', 'trocou', 'feito', 'bela', 'gostava', 'cuidado', 'numa', 'totalmente', 'votos', 'atitudes', 'teria', 'ponto', 'todo', 'trabalhos', 'boa', 'garota', 'muitos', 'sabe', 'tipo', 'AC', 'históricos', 'suava', 'fios', 'branca', 'dão', 'comparou', 'daquela', 'gostaria', 'Armando', 'las', 'sofreu', 'vestidos', 'dali', 'moral', 'nunca', 'quais', 'primeiro', 'superior', 'rapaz', 'juntou', 'vermelha', 'direitos', 'haver', 'inteira', 'todas', 'ela', 'motivos', 'senhora', 'mortos', 'ex', 'acorda', 'atribuiu', 'de', 'vales', 'Ana', 'seguinte', 'negativos', 'legais', 'brancos', 'imagens', 'notou', 'cabo', 'Chico', 'centenas', 'dando', 'encontrada', 'obras', 'vaga', 'abaixo', 'fala', 'taxa', 'alma', 'radia', 'aquela', 'características', 'deus', 'pelos', 'sentidos', 'sentido', 'instrumentos', 'argentina', 'promete', 'recursos', 'latina', 'atender', 'nossos', 'rapa', 'até', 'teve', 'vizinha', 'lunar', 'irmã', 'divina', 'cada', 'tem', 'ar', 'fraca', 'resultados', 'compra', 'ente', 'sobe', 'talo', 'somente', 'Brasil', 'vítima', 'textos', 'vinda', 'espécie', 'dados', 'talentos', 'graças', 'ara', 'daquele', 'chamada', 'lugares', 'palavras', 'programas', 'positivos', 'houve', 'mesmos', 'd', 'ficam', 'ouvidos', 'ria', 'parte', 'típicos', 'verdadeira', 'contou', 'causou', 'grega', 'planos', 'alguma', 'noite', 'ódio', 'vez', 'extra', 'visual', 'seus', 'juventude', 'caminhos', 'graça', 'bateu', 'dupla', 'canais', 'culpa', 'reta', 'etc', 'verdes', 'obteve', 'abriu', 'vai', 'tempos', 'pensou', 'pagou', 'papéis', 'anti', 'dele', 'nestes', 'mente', 'temos', 'é', 'dentes', 'próprio', 'altos', 'vi', 'santa', 'voltar', 'gerou', 'suspeita', 'garantiu', 'carne', 'alguns', 'capazes', 'igual', 'chamou', 'porém', 'dona', 'mil', 'velhos', 'ou', 'enormes', 'corpo', 'na', 'oposta', 'perceber', 'seres', 'mínimos', 'eterna', 'num', 'vermelhos', 'meio', 'multi', 'demorou', 'compartilha', 'desta', 'americana', 'pessoa', 'dor', 'à', 'tanto', 'la', 'reconhecer', 'irmãos', 'mesmo', 'pegou', 'ataques', 'lhe', 'uma', 'nomes', 'cobra', 'perdidos', 'homens', 'que', 'percebeu', 'milhares', 'tiver', 'valores', 'paga', 'aparentemente', 'tala', 'facilidade', 'lançou', 'menina', 'sozinha', 'todos', 'sem', 'organizada', 'estabeleceu', 'filhos', 'comunistas', 'disso', 'além', 'juntos', 'e', 'Jesus', 'órgãos', 'publica']\n",
        "#ime_vocab"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qlUEMA1fEUdt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "7bbe4646-dd80-4f77-85bb-149a0567f0d1"
      },
      "source": [
        "s = '?en!tão..'\n",
        "print(s.encode().decode('ascii', 'ignore'))\n",
        "print(re.sub(r'[^A-Za-z0-9]+', '', s))\n",
        "\n",
        "import unicodedata\n",
        "unicodedata.normalize('NFD', s).encode('ascii', 'ignore').decode()\n",
        "\n",
        "punctuation = ['.', ',', ':', ';', '?', '!']\n",
        "print(re.sub(r\"[.,!?;]\", '', s))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "?en!to..\n",
            "ento\n",
            "então\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dIcf7OTw3rd_",
        "colab_type": "text"
      },
      "source": [
        "# Datasets\n",
        "\n",
        "- REGRA in email\n",
        "- ParaCraw\n",
        "- [Others](https://lionbridge.ai/datasets/best-portuguese-language-datasets-for-machine-learning/)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "onL1k4qI2Z6Q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "a85223c7-68ec-42a5-e333-11cb0a0037ff"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BXM-yaR-1ZIa",
        "colab_type": "text"
      },
      "source": [
        "#### BlogsetBR"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pf6iJ6613wT0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ebaca1ec-12bd-49c5-ebcf-1a66db371628"
      },
      "source": [
        "# Download others\n",
        "!mkdir /content/blogsetBR\n",
        "!time wget -nc --progress=dot:giga -P /content/blogsetBR \\\n",
        "    -i \"http://www.inf.pucrs.br/linatural/blogs/blogset-br.csv.gz\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-06-22 01:54:18--  http://www.inf.pucrs.br/linatural/blogs/blogset-br.csv.gz\n",
            "Resolving www.inf.pucrs.br (www.inf.pucrs.br)... 104.18.21.134, 104.18.20.134, 2606:4700::6812:1586, ...\n",
            "Connecting to www.inf.pucrs.br (www.inf.pucrs.br)|104.18.21.134|:80... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://www.inf.pucrs.br/linatural/blogs/blogset-br.csv.gz [following]\n",
            "--2020-06-22 01:54:18--  https://www.inf.pucrs.br/linatural/blogs/blogset-br.csv.gz\n",
            "Connecting to www.inf.pucrs.br (www.inf.pucrs.br)|104.18.21.134|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5015788144 (4.7G) [application/x-gzip]\n",
            "Saving to: ‘/content/blogsetBR/blogset-br.csv.gz’\n",
            "\n",
            "     0K ........ ........ ........ ........  0% 7.44M 10m38s\n",
            " 32768K ........ ........ ........ ........  1% 9.75M 9m19s\n",
            " 65536K ........ ........ ........ ........  2% 9.75M 8m50s\n",
            " 98304K ........ ........ ........ ........  2% 8.69M 8m49s\n",
            "131072K ........ ........ ........ ........  3% 9.75M 8m35s\n",
            "163840K ........ ........ ........ ........  4% 9.75M 8m25s\n",
            "196608K ........ ........ ........ ........  4% 9.75M 8m16s\n",
            "229376K ........ ........ ........ ........  5% 8.68M 8m17s\n",
            "262144K ........ ........ ........ ........  6% 9.75M 8m9s\n",
            "294912K ........ ........ ........ ........  6% 9.75M 8m3s\n",
            "327680K ........ ........ ........ ........  7% 9.75M 7m57s\n",
            "360448K ........ ........ ........ ........  8% 9.75M 7m52s\n",
            "393216K ........ ........ ........ ........  8% 8.68M 7m51s\n",
            "425984K ........ ........ ........ ........  9% 9.76M 7m46s\n",
            "458752K ........ ........ ........ ........ 10% 9.74M 7m41s\n",
            "491520K ........ ........ ........ ........ 10% 9.75M 7m37s\n",
            "524288K ........ ........ ........ ........ 11% 9.75M 7m32s\n",
            "557056K ........ ........ ........ ........ 12% 9.75M 7m28s\n",
            "589824K ........ ........ ........ ........ 12% 9.74M 7m24s\n",
            "622592K ........ ........ ........ ........ 13% 9.76M 7m19s\n",
            "655360K ........ ........ ........ ........ 14% 9.75M 7m15s\n",
            "688128K ........ ........ ........ ........ 14% 9.75M 7m11s\n",
            "720896K ........ ........ ........ ........ 15% 8.73M 7m9s\n",
            "753664K ........ ........ ........ ........ 16% 9.75M 7m5s\n",
            "786432K ........ ........ ........ ........ 16% 9.75M 7m2s\n",
            "819200K ........ ........ ........ ........ 17% 9.73M 6m58s\n",
            "851968K ........ ........ ........ ........ 18% 8.67M 6m56s\n",
            "884736K ........ ........ ........ ........ 18% 9.75M 6m52s\n",
            "917504K ........ ........ ........ ........ 19% 9.75M 6m48s\n",
            "950272K ........ ........ ........ ........ 20% 9.76M 6m44s\n",
            "983040K ........ ........ ........ ........ 20% 9.74M 6m40s\n",
            "1015808K ........ ........ ........ ........ 21% 9.75M 6m37s\n",
            "1048576K ........ ........ ........ ........ 22% 9.73M 6m33s\n",
            "1081344K ........ ........ ........ ........ 22% 9.77M 6m29s\n",
            "1114112K ........ ........ ........ ........ 23% 9.75M 6m26s\n",
            "1146880K ........ ........ ........ ........ 24% 8.68M 6m23s\n",
            "1179648K ........ ........ ........ ........ 24% 9.73M 6m20s\n",
            "1212416K ........ ........ ........ ........ 25% 9.77M 6m16s\n",
            "1245184K ........ ........ ........ ........ 26% 9.75M 6m12s\n",
            "1277952K ........ ........ ........ ........ 26% 9.76M 6m9s\n",
            "1310720K ........ ........ ........ ........ 27% 9.75M 6m5s\n",
            "1343488K ........ ........ ........ ........ 28% 8.68M 6m3s\n",
            "1376256K ........ ........ ........ ........ 28% 9.17M 5m59s\n",
            "1409024K ........ ........ ........ ........ 29% 9.75M 5m56s\n",
            "1441792K ........ ........ ........ ........ 30% 9.75M 5m52s\n",
            "1474560K ........ ........ ........ ........ 30% 8.68M 5m50s\n",
            "1507328K ........ ........ ........ ........ 31% 9.76M 5m46s\n",
            "1540096K ........ ........ ........ ........ 32% 9.75M 5m42s\n",
            "1572864K ........ ........ ........ ........ 32% 9.75M 5m39s\n",
            "1605632K ........ ........ ........ ........ 33% 9.74M 5m35s\n",
            "1638400K ........ ........ ........ ........ 34% 8.68M 5m33s\n",
            "1671168K ........ ........ ........ ........ 34% 9.74M 5m29s\n",
            "1703936K ........ ........ ........ ........ 35% 9.76M 5m25s\n",
            "1736704K ........ ........ ........ ........ 36% 9.75M 5m22s\n",
            "1769472K ........ ........ ........ ........ 36% 9.75M 5m18s\n",
            "1802240K ........ ........ ........ ........ 37% 8.74M 5m16s\n",
            "1835008K ........ ........ ........ ........ 38% 9.78M 5m12s\n",
            "1867776K ........ ........ ........ ........ 38% 9.08M 5m9s\n",
            "1900544K ........ ........ ........ ........ 39% 9.75M 5m5s\n",
            "1933312K ........ ........ ........ ........ 40% 9.75M 5m2s\n",
            "1966080K ........ ........ ........ ........ 40% 9.75M 4m58s\n",
            "1998848K ........ ........ ........ ........ 41% 8.72M 4m55s\n",
            "2031616K ........ ........ ........ ........ 42% 9.74M 4m52s\n",
            "2064384K ........ ........ ........ ........ 42% 9.75M 4m48s\n",
            "2097152K ........ ........ ........ ........ 43% 9.75M 4m45s\n",
            "2129920K ........ ........ ........ ........ 44% 9.75M 4m41s\n",
            "2162688K ........ ........ ........ ........ 44% 8.97M 4m38s\n",
            "2195456K ........ ........ ........ ........ 45% 8.67M 4m35s\n",
            "2228224K ........ ........ ........ ........ 46% 9.75M 4m32s\n",
            "2260992K ........ ........ ........ ........ 46% 9.14M 4m29s\n",
            "2293760K ........ ........ ........ ........ 47% 8.67M 4m26s\n",
            "2326528K ........ ........ ........ ........ 48% 9.76M 4m22s\n",
            "2359296K ........ ........ ........ ........ 48% 9.75M 4m19s\n",
            "2392064K ........ ........ ........ ........ 49% 9.76M 4m15s\n",
            "2424832K ........ ........ ........ ........ 50% 9.74M 4m12s\n",
            "2457600K ........ ........ ........ ........ 50% 9.76M 4m8s\n",
            "2490368K ........ ........ ........ ........ 51% 9.74M 4m5s\n",
            "2523136K ........ ........ ........ ........ 52% 9.75M 4m1s\n",
            "2555904K ........ ........ ........ ........ 52% 8.68M 3m58s\n",
            "2588672K ........ ........ ........ ........ 53% 9.71M 3m55s\n",
            "2621440K ........ ........ ........ ........ 54% 9.78M 3m51s\n",
            "2654208K ........ ........ ........ ........ 54% 8.68M 3m48s\n",
            "2686976K ........ ........ ........ ........ 55% 9.75M 3m45s\n",
            "2719744K ........ ........ ........ ........ 56% 9.74M 3m41s\n",
            "2752512K ........ ........ ........ ........ 56% 9.76M 3m38s\n",
            "2785280K ........ ........ ........ ........ 57% 8.67M 3m35s\n",
            "2818048K ........ ........ ........ ........ 58% 9.74M 3m31s\n",
            "2850816K ........ ........ ........ ........ 58% 9.77M 3m28s\n",
            "2883584K ........ ........ ........ ........ 59% 9.75M 3m24s\n",
            "2916352K ........ ........ ........ ........ 60% 9.75M 3m21s\n",
            "2949120K ........ ........ ........ ........ 60% 9.74M 3m17s\n",
            "2981888K ........ ........ ........ ........ 61% 9.75M 3m14s\n",
            "3014656K ........ ........ ........ ........ 62% 9.71M 3m10s\n",
            "3047424K ........ ........ ........ ........ 62% 8.71M 3m7s\n",
            "3080192K ........ ........ ........ ........ 63% 9.75M 3m4s\n",
            "3112960K ........ ........ ........ ........ 64% 9.75M 3m0s\n",
            "3145728K ........ ........ ........ ........ 64% 8.68M 2m57s\n",
            "3178496K ........ ........ ........ ........ 65% 9.75M 2m54s\n",
            "3211264K ........ ........ ........ ........ 66% 9.76M 2m50s\n",
            "3244032K ........ ........ ........ ........ 66% 8.68M 2m47s\n",
            "3276800K ........ ........ ........ ........ 67% 9.76M 2m44s\n",
            "3309568K ........ ........ ........ ........ 68% 9.75M 2m40s\n",
            "3342336K ........ ........ ........ ........ 68% 9.75M 2m37s\n",
            "3375104K ........ ........ ........ ........ 69% 8.68M 2m34s\n",
            "3407872K ........ ........ ........ ........ 70% 9.74M 2m30s\n",
            "3440640K ........ ........ ........ ........ 70% 9.76M 2m27s\n",
            "3473408K ........ ........ ........ ........ 71% 9.75M 2m23s\n",
            "3506176K ........ ........ ........ ........ 72% 9.75M 2m20s\n",
            "3538944K ........ ........ ........ ........ 72% 8.73M 2m17s\n",
            "3571712K ........ ........ ........ ........ 73% 9.69M 2m13s\n",
            "3604480K ........ ........ ........ ........ 74% 9.75M 2m10s\n",
            "3637248K ........ ........ ........ ........ 74% 9.75M 2m6s\n",
            "3670016K ........ ........ ........ ........ 75% 8.68M 2m3s\n",
            "3702784K ........ ........ ........ ........ 76% 9.74M 2m0s\n",
            "3735552K ........ ........ ........ ........ 76% 9.75M 1m56s\n",
            "3768320K ........ ........ ........ ........ 77% 9.76M 1m53s\n",
            "3801088K ........ ........ ........ ........ 78% 8.72M 1m50s\n",
            "3833856K ........ ........ ........ ........ 78% 9.75M 1m46s\n",
            "3866624K ........ ........ ........ ........ 79% 9.75M 1m43s\n",
            "3899392K ........ ........ ........ ........ 80% 9.75M 1m40s\n",
            "3932160K ........ ........ ........ ........ 80% 8.67M 96s\n",
            "3964928K ........ ........ ........ ........ 81% 9.75M 93s\n",
            "3997696K ........ ........ ........ ........ 82% 9.75M 89s\n",
            "4030464K ........ ........ ........ ........ 82% 9.75M 86s\n",
            "4063232K ........ ........ ........ ........ 83% 9.76M 83s\n",
            "4096000K ........ ........ ........ ........ 84% 9.75M 79s\n",
            "4128768K ........ ........ ........ ........ 84% 8.68M 76s\n",
            "4161536K ........ ........ ........ ........ 85% 9.74M 73s\n",
            "4194304K ........ ........ ........ ........ 86% 9.73M 69s\n",
            "4227072K ........ ........ ........ ........ 86% 9.75M 66s\n",
            "4259840K ........ ........ ........ ........ 87% 8.70M 62s\n",
            "4292608K ........ ........ ........ ........ 88% 9.75M 59s\n",
            "4325376K ........ ........ ........ ........ 88% 9.75M 56s\n",
            "4358144K ........ ........ ........ ........ 89% 9.75M 52s\n",
            "4390912K ........ ........ ........ ........ 90% 9.75M 49s\n",
            "4423680K ........ ........ ........ ........ 90% 8.69M 46s\n",
            "4456448K ........ ........ ........ ........ 91% 9.75M 42s\n",
            "4489216K ........ ........ ........ ........ 92% 9.75M 39s\n",
            "4521984K ........ ........ ........ ........ 92% 9.76M 35s\n",
            "4554752K ........ ........ ........ ........ 93% 9.74M 32s\n",
            "4587520K ........ ........ ........ ........ 94% 8.94M 29s\n",
            "4620288K ........ ........ ........ ........ 94% 9.76M 25s\n",
            "4653056K ........ ........ ........ ........ 95% 9.75M 22s\n",
            "4685824K ........ ........ ........ ........ 96% 8.68M 19s\n",
            "4718592K ........ ........ ........ ........ 97% 9.75M 15s\n",
            "4751360K ........ ........ ........ ........ 97% 9.74M 12s\n",
            "4784128K ........ ........ ........ ........ 98% 9.72M 8s\n",
            "4816896K ........ ........ ........ ........ 99% 9.79M 5s\n",
            "4849664K ........ ........ ........ ........ 99% 9.75M 2s\n",
            "4882432K ........ .......                   100% 9.75M=8m24s\n",
            "\n",
            "2020-06-22 02:02:43 (9.49 MB/s) - ‘/content/blogsetBR/blogset-br.csv.gz’ saved [5015788144/5015788144]\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "UnicodeDecodeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-c703b42de9e9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Download others\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mkdir /content/blogsetBR'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'time wget -nc --progress=dot:giga -P /content/blogsetBR     -i http://www.inf.pucrs.br/linatural/blogs/blogset-br.csv.gz'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/_shell.py\u001b[0m in \u001b[0;36msystem\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    100\u001b[0m       \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'also_return_output'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_system_commands\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_system_compat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint:disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpip_warn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36m_system_compat\u001b[0;34m(shell, cmd, also_return_output)\u001b[0m\n\u001b[1;32m    436\u001b[0m   \u001b[0;31m# stack.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m   result = _run_command(\n\u001b[0;32m--> 438\u001b[0;31m       shell.var_expand(cmd, depth=2), clear_streamed_output=False)\n\u001b[0m\u001b[1;32m    439\u001b[0m   \u001b[0mshell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_ns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'_exit_code'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_INTERRUPTED_SIGNALS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36m_run_command\u001b[0;34m(cmd, clear_streamed_output)\u001b[0m\n\u001b[1;32m    193\u001b[0m       \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchild_pty\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 195\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0m_monitor_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparent_pty\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoll\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdate_stdin_widget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m   \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0mepoll\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36m_monitor_process\u001b[0;34m(parent_pty, epoll, p, cmd, update_stdin_widget)\u001b[0m\n\u001b[1;32m    220\u001b[0m   \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 222\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_poll_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparent_pty\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoll\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    223\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36m_poll_process\u001b[0;34m(parent_pty, epoll, p, cmd, decoder, state)\u001b[0m\n\u001b[1;32m    273\u001b[0m       \u001b[0moutput_available\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m       \u001b[0mraw_contents\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparent_pty\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_PTY_READ_MAX_BYTES_FOR_TEST\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 275\u001b[0;31m       \u001b[0mdecoded_contents\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_contents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    276\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m       \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecoded_contents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/codecs.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, input, final)\u001b[0m\n\u001b[1;32m    319\u001b[0m         \u001b[0;31m# decode input (taking the buffer into account)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuffer\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 321\u001b[0;31m         \u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconsumed\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_buffer_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    322\u001b[0m         \u001b[0;31m# keep undecoded input until the next call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuffer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mconsumed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0xab in position 57: invalid start byte"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rxBI9HplxNgI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377
        },
        "outputId": "3571b411-c259-41e9-bd03-69694ee4d064"
      },
      "source": [
        "import pandas as pd\n",
        "posts = pd.read_csv('/content/blogsetBR/blogset-br.csv.gz', compression='gzip',\n",
        "                    nrows=20, header=True)\n",
        "posts.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-ed324c11bd59>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m posts = pd.read_csv('/content/blogsetBR/blogset-br.csv.gz', compression='gzip',\n\u001b[0;32m----> 3\u001b[0;31m                     nrows=20, header=True)\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mposts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    674\u001b[0m         )\n\u001b[1;32m    675\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 676\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    678\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 448\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    449\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    873\u001b[0m         \u001b[0;31m# might mutate self.engine\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    874\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_file_or_buffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 875\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_clean_options\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    876\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    877\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"has_index_names\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_clean_options\u001b[0;34m(self, options, engine)\u001b[0m\n\u001b[1;32m   1040\u001b[0m         \u001b[0mskiprows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"skiprows\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1042\u001b[0;31m         \u001b[0mvalidate_header_arg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"header\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1043\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1044\u001b[0m         \u001b[0mdepr_warning\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mvalidate_header_arg\u001b[0;34m(header)\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mheader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         raise TypeError(\n\u001b[0;32m---> 81\u001b[0;31m             \u001b[0;34m\"Passing a bool to header is invalid. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m             \u001b[0;34m\"Use header=None for no header or \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m             \u001b[0;34m\"header=int or list-like of ints to specify \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: Passing a bool to header is invalid. Use header=None for no header or header=int or list-like of ints to specify the row(s) making up the column names"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Oaw4fOy1dX1",
        "colab_type": "text"
      },
      "source": [
        "### Brazilian Portuguese Literature Corpus"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ruRPH3eo1kci",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "5b693874-f690-44d2-b6fd-e21f1d497f38"
      },
      "source": [
        "!pip3 install --quiet kaggle\n",
        "!ls -a ~\n",
        "!cp \"/content/drive/My Drive/kaggle.json\" ~/.kaggle\n",
        "!ls -a ~/.kaggle\n",
        "!kaggle datasets download --unzip \\\n",
        "  -d rtatman/brazilian-portuguese-literature-corpus"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ".   .bash_history  .cache   .gsutil   .jupyter\t.keras\t.node-gyp  .profile\n",
            "..  .bashrc\t   .config  .ipython  .kaggle\t.local\t.npm\t   .wget-hsts\n",
            ".  ..  kaggle.json\n",
            "Downloading brazilian-portuguese-literature-corpus.zip to /content\n",
            " 29% 5.00M/17.5M [00:00<00:00, 22.6MB/s]\n",
            "100% 17.5M/17.5M [00:00<00:00, 58.2MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hWaZBlpt6khQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "path = \"/content/Brazilian_Portugese_Corpus\"\n",
        "br_litteture_corpus = []\n",
        "for root, dirs, files in os.walk(path):\n",
        "  for file in files:\n",
        "    if file.endswith(\".txt\"):\n",
        "      path_file = os.path.join(root, file)\n",
        "      with open(path_file, encoding=\"latin1\") as txt_file: # Use file to refer to the file object\n",
        "        br_litteture_corpus.append(txt_file.read().splitlines())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9HpIh23T-b9X",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "d6a28dad-9c51-4dee-d671-20fc3527434b"
      },
      "source": [
        "br_litteture = [seq.split() for filecorpus in br_litteture_corpus for seq in filecorpus]\n",
        "print('numero de sentenças:', len(br_litteture))\n",
        "print(br_litteture[0])\n",
        "words = [word for seq in br_litteture for word in seq]\n",
        "print(words[0])\n",
        "print('numero de palavras:', len(words))\n",
        "non_norm_corpus_vocab = list(set(words))\n",
        "print('numero de palavra unicas', len(non_norm_corpus_vocab))\n",
        "\n",
        "print('\\n...normalizando...\\n')\n",
        "norm_br_litteture = [seq.lower().split() for filecorpus in br_litteture_corpus for seq in filecorpus]\n",
        "print(norm_br_litteture[0])\n",
        "print('numero de sentenças:', len(norm_br_litteture))\n",
        "norm_words = [word for seq in norm_br_litteture for word in seq]\n",
        "print(norm_words[0])\n",
        "print('numero de palavras:', len(norm_words))\n",
        "norm_corpus_vocab = list(set(norm_words))\n",
        "print('numero de palavra unicas', len(norm_corpus_vocab))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "numero de sentenças: 886562\n",
            "['Uma', 'Lágrima', 'de', 'Mulher']\n",
            "Uma\n",
            "numero de palavras: 7594582\n",
            "numero de palavra unicas 233317\n",
            "\n",
            "...normalizando...\n",
            "\n",
            "['uma', 'lágrima', 'de', 'mulher']\n",
            "numero de sentenças: 886562\n",
            "uma\n",
            "numero de palavras: 7594582\n",
            "numero de palavra unicas 214783\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "TgUkD55Dkam2"
      },
      "source": [
        "## Classe Dataset\n",
        "Gerenciamento dos dados, e um pequeno teste.\n",
        "No getitem é aplicada a correção de codificação."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "R2GaMtgRkaze",
        "colab": {}
      },
      "source": [
        "class BrazilianLiterature(Dataset):\n",
        "    \"\"\"\n",
        "    Loads data from preprocessed file and manages them.\n",
        "    \"\"\"\n",
        "    VALID_MODES = [\"train\", \"validation\", \"test\"]\n",
        "    TOKENIZER = T5Tokenizer.from_pretrained(hparams[\"model_name\"],\n",
        "                                            cache_dir=base_path)\n",
        "    def __init__(self, mode: str, seq_len: int):\n",
        "        \"\"\"\n",
        "        mode: One of train, validation or test \n",
        "        seq_len: limit to returned encoded tokens\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        assert mode in BEA2019.VALID_MODES\n",
        "\n",
        "        self.mode = mode\n",
        "        self.seq_len = seq_len\n",
        "\n",
        "        file_name = os.path.join(base_path, f\"{mode}.pkl\")\n",
        "        if not os.path.isfile(file_name):\n",
        "            print(\"Pre-processed files not found, preparing data.\")\n",
        "            self.prepare_data()\n",
        "        \n",
        "        with open(file_name, \"rb\") as preprocessed_file:\n",
        "            self.data = pickle.load(preprocessed_file)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, i: int):\n",
        "        \"\"\"\n",
        "        Unpacks line from data and applies T5 encoding if necessary.\n",
        "\n",
        "        returns: input_ids, lm_labels, original, corrected\n",
        "        \"\"\"\n",
        "        input, target, original, corrected = self.data[i]\n",
        "\n",
        "        # From Leard's solution\n",
        "        # target = unicodedata.normalize(\"NFD\", target).encode(\"latin-1\", \"xmlcharrefreplace\").decode(\"latin-1\")\n",
        "\n",
        "        eos_token = BEA2019.TOKENIZER.eos_token\n",
        "\n",
        "        inputs = BEA2019.TOKENIZER.encode_plus(text=f\"{input}\",\n",
        "                                               max_length=self.seq_len,\n",
        "                                               pad_to_max_length=True,\n",
        "                                               return_tensors=\"pt\")\n",
        "\n",
        "        lm_labels = BEA2019.TOKENIZER.encode_plus(text=f\"{target}{eos_token}\",\n",
        "                                                  max_length=self.seq_len,\n",
        "                                                  pad_to_max_length=True,\n",
        "                                                  return_tensors=\"pt\")\n",
        "        return inputs[\"input_ids\"].squeeze(), lm_labels[\"input_ids\"].squeeze(), original, corrected\n",
        "\n",
        "    def get_dataloader(self, batch_size: int, shuffle: bool):\n",
        "        return DataLoader(self, batch_size=batch_size, shuffle=shuffle, \n",
        "                          num_workers=4)\n",
        "\n",
        "    @staticmethod\n",
        "    def load_text_tuples(path, member):\n",
        "        \"\"\"\n",
        "        Load tuples from original files: text_original, text_input, text_target, text_corrected.\n",
        "        \"\"\"\n",
        "        text_tuples = []\n",
        "        with tarfile.open(path) as tar:\n",
        "            f = tar.extractfile(member)\n",
        "            for line in f:\n",
        "                data = json.loads(line)\n",
        "                if len(data[\"text\"]) > 200:\n",
        "                    continue\n",
        "                # list of edits, one for each annotators\n",
        "                for annotator_id, edits in data[\"edits\"]:\n",
        "                    # edit: [annotator_id, [[char_start_offset, char_end_offset, correction], ...]]\n",
        "                    text_input = \"\"\n",
        "                    text_target = \"\"\n",
        "                    text_original = data[\"text\"]\n",
        "                    text_corrected = \"\"\n",
        "                    prev_char_end_offset = 0\n",
        "\n",
        "                    for idx, (char_start_offset, char_end_offset, correction) in enumerate(edits):\n",
        "                        # a slice of unchanged original text \n",
        "                        text_unchanged = text_original[prev_char_end_offset:char_start_offset]\n",
        "\n",
        "                        # a mask token\n",
        "                        mask = f\"<extra_id_{idx+1}>\"\n",
        "\n",
        "                        # the input for T5 model\n",
        "                        text_input = f\"{text_input} {text_unchanged} {mask}\"\n",
        "\n",
        "                        # the output for T5 model\n",
        "                        if correction is None:\n",
        "                            correction = \"\"\n",
        "                        text_target = f\"{text_target} {mask} {correction}\"\n",
        "\n",
        "                        text_corrected = f\"{text_corrected} {text_unchanged}{correction}\"\n",
        "\n",
        "                        prev_char_end_offset = char_end_offset\n",
        "\n",
        "                    text_unchanged = text_original[prev_char_end_offset:]\n",
        "                    mask = f\"<extra_id_{idx+1}>\"\n",
        "\n",
        "                    text_input = f\"{text_input} {text_unchanged}\".lstrip()\n",
        "                    text_target = f\"{text_target} {mask}\".lstrip()\n",
        "                    text_corrected = f\"{text_corrected} {text_unchanged}\".lstrip()\n",
        "                    text_tuples.append((text_input, text_target, text_original, text_corrected))\n",
        "\n",
        "        return text_tuples\n",
        "\n",
        "    @staticmethod\n",
        "    def prepare_data(train_size=10, val_size=3):\n",
        "        \"\"\"\n",
        "        Performs everything needed to get the data ready.\n",
        "        Addition of Eos token and encoding is performed in runtime.\n",
        "        \"\"\"\n",
        "        if not os.path.isfile(\"wi+locness_v2.1.bea19.tar.gz\"):    \n",
        "            !wget -nc https://www.cl.cam.ac.uk/research/nl/bea2019st/data/wi+locness_v2.1.bea19.tar.gz -P \"$BASE_PATH\"\n",
        "            # !wget -nc https://www.cl.cam.ac.uk/research/nl/bea2019st/data/ABCN.bea19.dev.orig -P \"$BASE_PATH\"\n",
        "            # !wget -nc https://www.cl.cam.ac.uk/research/nl/bea2019st/data/ABCN.bea19.test.orig -P \"$BASE_PATH\"\n",
        "\n",
        "        data = {}\n",
        "        train_val_data = BEA2019.load_text_tuples(os.path.join(base_path, \"wi+locness_v2.1.bea19.tar.gz\"), \"wi+locness/json/A.train.json\")\n",
        "        test_data = BEA2019.load_text_tuples(os.path.join(base_path, \"wi+locness_v2.1.bea19.tar.gz\"), \"wi+locness/json/A.dev.json\")\n",
        "\n",
        "        random.shuffle(train_val_data)\n",
        "\n",
        "        train_data = train_val_data[:train_size]\n",
        "        val_data = train_val_data[train_size:train_size + val_size]\n",
        "\n",
        "        for mode, data in zip(BEA2019.VALID_MODES, [train_data, val_data, test_data]):\n",
        "            file_name = os.path.join(base_path, f\"{mode}.pkl\")\n",
        "            with open(file_name, \"wb\") as pkl_file:\n",
        "                pickle.dump(data, pkl_file)\n",
        "            print(f\"Pre-processed data saved as {file_name}.\")\n",
        "\n",
        "\n",
        "datasets = {m: BEA2019(mode=m, seq_len=hparams[\"seq_len\"]) for m in BEA2019.VALID_MODES}\n",
        "\n",
        "# Testing datasets\n",
        "for mode, dataset in datasets.items():\n",
        "    print(f\"\\n{mode} dataset length: {len(dataset)}\\n\")\n",
        "    print(\"Random sample\")\n",
        "    input_ids, lm_labels, original, corrected = random.choice(dataset)\n",
        "    print(\"input_ids\\n\", input_ids, end=\"\\n\\n\")\n",
        "    print(\"lm_labels\\n\", lm_labels, end=\"\\n\\n\")\n",
        "    print(\"original\\n\", original, end=\"\\n\\n\")\n",
        "    print(\"corrected\\n\", corrected, end=\"\\n\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cloyt0tIwIiD",
        "colab_type": "text"
      },
      "source": [
        "## Dataloaders\n",
        "\n",
        "Verificação se dataloaders estão funcionando corretamente."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZoKiQXCvwGrP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "shuffle = {\"train\": True, \"validation\": False, \"test\": False}\n",
        "debug_dataloaders = {mode: datasets[mode].get_dataloader(batch_size=hparams[\"batch_size\"], \n",
        "                                                         shuffle=shuffle[mode])\n",
        "                     for mode in BEA2019.VALID_MODES}\n",
        "\n",
        "# Testing dataloaders\n",
        "for mode, dataloader in debug_dataloaders.items():\n",
        "    print(\"{} number of batches: {}\".format(mode, len(dataloader)))\n",
        "    batch = next(iter(dataloader))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YGH2rc3lthSB",
        "colab_type": "text"
      },
      "source": [
        "## Lightning Module\n",
        "\n",
        "Aqui a classe principal do PyTorch Lightning é definida.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jg-hZEktbvnr",
        "colab": {}
      },
      "source": [
        "class T5Corrector(pl.LightningModule):\n",
        "    def __init__(self, hparams):\n",
        "        super().__init__()\n",
        "\n",
        "        self.hparams = hparams\n",
        "        self.t5 = T5ForConditionalGeneration.from_pretrained(hparams.model_name,\n",
        "                                                             cache_dir=hparams.base_path)\n",
        "        self.tokenizer = BEA2019.TOKENIZER\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Inspirado pela solução do Lucas\n",
        "        \"\"\"\n",
        "        input_ids, lm_labels, original, corrected = x\n",
        "\n",
        "        if self.training:\n",
        "            return self.t5(input_ids=input_ids,\n",
        "                           lm_labels=lm_labels)[0]\n",
        "        else:   \n",
        "            # or use transformers library decoding\n",
        "            if self.hparams.decode_mode == \"greedy\":\n",
        "                return self.t5.generate(input_ids=input_ids,\n",
        "                                        lm_labels=lm_labels), original, corrected\n",
        "            elif self.hparams.decode_mode == \"beam\":\n",
        "                return self.t5.generate(input_ids=input_ids,\n",
        "                                        lm_labels=lm_labels,\n",
        "                                        num_beams=self.hparams.nbeams, \n",
        "                                        num_return_sequences=10,\n",
        "                                        early_stopping=True), original, corrected\n",
        "            elif self.hparams.decode_mode == \"topk\":\n",
        "                return self.t5.generate(input_ids=input_ids,\n",
        "                                        lm_labels=lm_labels,\n",
        "                                        num_return_sequences=10,\n",
        "                                        top_k=self.hparams.k, \n",
        "                                        do_sample=True), original, corrected\n",
        "            elif self.hparams.decode_mode == \"nucleus\":\n",
        "                return self.t5.generate(input_ids=input_ids,\n",
        "                                        lm_labels=lm_labels,\n",
        "                                        top_k=0, \n",
        "                                        top_p=self.hparams.p,\n",
        "                                        do_sample=True), original, corrected\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        loss = self(batch)\n",
        "\n",
        "        return {\"loss\": loss, \"log\": {\"loss\": loss}, \"progress_bar\": hardware_stats()}\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        pred_tokens, original, corrected = self(batch)\n",
        "\n",
        "        # List of string predictions, one per item of mini_batch\n",
        "        pred = [html.unescape(self.tokenizer.decode(pred)) for pred in pred_tokens]\n",
        "    \n",
        "        return {\"pred\": pred, \"corrected\": corrected, \"progress_bar\":  hardware_stats()}\n",
        "\n",
        "    def test_step(self, batch, batch_idx):\n",
        "        pred_tokens, original, corrected = self(batch)\n",
        "\n",
        "        # List of string predictions, one per item of mini_batch\n",
        "        pred = [html.unescape(self.tokenizer.decode(pred)) for pred in pred_tokens]\n",
        "\n",
        "        return {\"pred\": pred, \"corrected\": corrected, \"progress_bar\":  hardware_stats()}\n",
        "\n",
        "    def training_epoch_end(self, outputs):\n",
        "        avg_loss = torch.stack([x[\"loss\"] for x in outputs]).mean()\n",
        "\n",
        "        return {\"log\": {\"train_loss\": avg_loss}} \n",
        "\n",
        "    def validation_epoch_end(self, outputs):\n",
        "        \"\"\"\n",
        "        Inspirado pela solução do Israel\n",
        "        \"\"\"\n",
        "        trues = sum([list(x[\"corrected\"]) for x in outputs], [])\n",
        "        preds = sum([list(x[\"pred\"]) for x in outputs], [])\n",
        "\n",
        "        n = random.choice(range(len(trues)))\n",
        "        print(f\"\\nTarget: {trues[n]}\\nPrediction: {preds[n]}\\n\")\n",
        "\n",
        "        # annotator = errant.load(\"en\")\n",
        "        # orig = annotator.parse(original[0])\n",
        "        # cor = annotator.parse(pred)\n",
        "        # print(\"orig\", orig)\n",
        "        # print(\"cor\", cor)\n",
        "        # annotator.annotate(orig, cor, lev=False, merging=\"rules\")\n",
        "\n",
        "        f_score = 0.5 #TODO\n",
        "        val_dict = {\"f_score\": f_score}\n",
        "    \n",
        "        return {\"f_score\": f_score, \"log\": val_dict, \"progress_bar\": val_dict}\n",
        "\n",
        "    def test_epoch_end(self, outputs):\n",
        "        trues = sum([list(x[\"corrected\"]) for x in outputs], [])\n",
        "        preds = sum([list(x[\"pred\"]) for x in outputs], [])\n",
        "\n",
        "        n = random.choice(range(len(trues)))\n",
        "        print(f\"\\nTarget: {trues[n]}\\nPrediction: {preds[n]}\\n\")\n",
        "        f_score = 0.5 # TODO\n",
        "        test_dict = {\"test_f_score\": f_score}\n",
        "    \n",
        "        return {\"test_f_score\": f_score, \"log\": test_dict, \"progress_bar\": test_dict}\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        return Adam(self.parameters(), lr=self.hparams.lr)    \n",
        "\n",
        "    def train_dataloader(self):\n",
        "        if self.hparams.overfit_pct > 0:\n",
        "            logging.info(\"Disabling train shuffle due to overfit_pct.\")\n",
        "            shuffle = False\n",
        "        else:\n",
        "            shuffle = True\n",
        "        dataset = BEA2019(\"train\", seq_len=self.hparams.seq_len)\n",
        "        return dataset.get_dataloader(batch_size=self.hparams.batch_size, shuffle=shuffle)\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        dataset = BEA2019(\"validation\", seq_len=self.hparams.test_seq_len)\n",
        "        return dataset.get_dataloader(batch_size=self.hparams.batch_size, shuffle=False)\n",
        "\n",
        "    def test_dataloader(self):\n",
        "        dataset = BEA2019(\"test\", seq_len=self.hparams.test_seq_len)\n",
        "        return dataset.get_dataloader(batch_size=self.hparams.batch_size, shuffle=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "YbA8e334UM-N"
      },
      "source": [
        "## Preparação e Testes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "XDAYv5_zURPc",
        "colab": {}
      },
      "source": [
        "hparams = {\"name\": experiment_name, \"base_path\": base_path,\n",
        "           \"model_name\": model_name, \"seq_len\": 200, \"test_seq_len\": 300,\n",
        "           \"lr\": learning_rate, \"batch_size\": batch_size, \"batch_accum\": accumulate_grad_batches,\n",
        "           \"max_epochs\": 3,\n",
        "           \"overfit_pct\": 0, \"debug\": 0,\n",
        "           \"decode_mode\": decode_mode}\n",
        "\n",
        "\n",
        "for key, parameter in hparams.items():\n",
        "    print(\"{}: {}\".format(key, parameter))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fsxBZdkDzhNQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Instantiate model\n",
        "model = T5Corrector(Namespace(**hparams))\n",
        "\n",
        "# Folder/path management, for logs and checkpoints\n",
        "tensorboard_path = \"logs\"\n",
        "experiment_name = hparams[\"name\"]\n",
        "model_folder = os.path.join(tensorboard_path, experiment_name)\n",
        "os.makedirs(model_folder, exist_ok=True)\n",
        "ckpt_path = os.path.join(model_folder, \"-{epoch}\")\n",
        "\n",
        "# Callback initialization\n",
        "checkpoint_callback = ModelCheckpoint(prefix=experiment_name, \n",
        "                                      filepath=ckpt_path, \n",
        "                                      mode=\"max\")\n",
        "logger = TensorBoardLogger(tensorboard_path, experiment_name)\n",
        "\n",
        "# PL Trainer initialization\n",
        "trainer = Trainer(gpus=1,\n",
        "                  checkpoint_callback=checkpoint_callback, \n",
        "                  early_stop_callback=False,\n",
        "                  logger=logger,\n",
        "                  accumulate_grad_batches=hparams[\"batch_accum\"],\n",
        "                  max_epochs=hparams[\"max_epochs\"], \n",
        "                  fast_dev_run=bool(hparams[\"debug\"]), \n",
        "                  overfit_pct=hparams[\"overfit_pct\"],\n",
        "                  progress_bar_refresh_rate=10,\n",
        "                  profiler=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lwtRpiSQCKa2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trainer.test(model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "efVSbZD1-chp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}