{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BEA2019_T5_NoTuning.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "bb99515e171745f3a765322dd58b8a76": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_05127b4eb5154f2888529d301561265b",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_9dad062bfea649399e92e4539d6acc30",
              "IPY_MODEL_9ea6d193ff5f415e82fe38141574ebd4"
            ]
          }
        },
        "05127b4eb5154f2888529d301561265b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": "row wrap",
            "width": "100%",
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": "inline-flex",
            "left": null
          }
        },
        "9dad062bfea649399e92e4539d6acc30": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_edc02f4530d14289aeeb88283df12550",
            "_dom_classes": [],
            "description": "Testing: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_1650921dad654f9abbc277741e643f9f"
          }
        },
        "9ea6d193ff5f415e82fe38141574ebd4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_b5efa7f90bee4c3f887587f96ddd9c0d",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 13/13 [1:16:12&lt;00:00, 351.73s/it]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_0f9f83fc2e7a4d0aa305c18fc25d3f5f"
          }
        },
        "edc02f4530d14289aeeb88283df12550": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "1650921dad654f9abbc277741e643f9f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": "2",
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "b5efa7f90bee4c3f887587f96ddd9c0d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "0f9f83fc2e7a4d0aa305c18fc25d3f5f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fberanizo/spelling-correction/blob/master/BEA2019_T5_NoTuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Up4u8rMkQSG",
        "colab_type": "text"
      },
      "source": [
        "# BEA 2019 Shared Task: Grammatical Error Correction: Modelo T5\n",
        "\n",
        "**Nome: Fabio Beranizo Lopes**<br>\n",
        "**Nome: Luiz Pita Almeida**\n",
        "\n",
        "Usaremos o modelo T5 prÃ©-treinado e o dataset W&I+LOCNESS do [Building Educational Applications 2019 Shared Task: Grammatical Error Correction](https://www.cl.cam.ac.uk/research/nl/bea2019st). <br>\n",
        "Truncamos para strings de tamanho 100 para deixar os testes mais rÃ¡pidos.\n",
        "\n",
        "MÃ©trica de avaliaÃ§Ã£o: F0.5-score <br>\n",
        "https://www.cl.cam.ac.uk/research/nl/bea2019st/#eval\n",
        "\n",
        "O mÃ©todo de correÃ§Ã£o aplicado foi sugerido pelos docentes:<br>\n",
        "> dado uma frase e um palavra nesta frase a ser corrigida ou nÃ£o, iremos\n",
        "> mascarar a palavra, rodar o BERT ou T5, e prever as top-10 palavras \n",
        "> alternativas usando mask language modeling. Se a palavra original estiver \n",
        "> entre as top previstas, nÃ£o sugerir correÃ§Ã£o. Caso contrÃ¡rio, usar edit \n",
        "> distance para ver qual Ã© a palavra mais prÃ³xima, e sugerÃ­-la ao usuÃ¡rio.\n",
        "\n",
        "Passos:\n",
        "\n",
        "1. Geram-se tuplas: `(original, corrected)`\n",
        "2. Aplica-se modelo T5 para prever top-10 palavras.<br>\n",
        "   Caso a palavra original esteja no Top 10 do modelo, Ã© classificada como correta.<br>\n",
        "   SenÃ£o, a palavra Ã© classificada como incorreta.\n",
        "\n",
        "**Obs: os notebooks contÃ©m excertos de cÃ³digos dos colegas de turma.**<br>\n",
        "**Obrigado Diedre, Gabriela, Leard, Lucas e Israel.**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CpELBvNmku5a",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "87c2a187-e873-4651-e91c-af9c15182cdc"
      },
      "source": [
        "import torch\n",
        "\n",
        "print(f\"Current GPU: {torch.cuda.get_device_name(0)}\")\n",
        "\n",
        "# don't even start if it's not a P100 GPU\n",
        "# if torch.cuda.get_device_name(0) != \"Tesla P100-PCIE-16GB\":\n",
        "#     import os\n",
        "#     os.kill(os.getpid(), 9)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Current GPU: Tesla P100-PCIE-16GB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FgW-boJLU0wU",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title ConfiguraÃ§Ãµes gerais\n",
        "experiment_name = \"no-tuning\"  #@param {type:\"string\"}\n",
        "model_name = \"t5-small\"  #@param [\"t5-small\", \"t5-base\", \"t5-large\", \"t5-3B\", \"t5-11B\"] {type:\"string\"}\n",
        "batch_size = 10  #@param {type:\"integer\"}\n",
        "accumulate_grad_batches = 1  #@param {type:\"integer\"}\n",
        "sequence_length = 1000  #@param {type:\"integer\"}\n",
        "learning_rate = 5e-3  #@param {type:\"number\"}\n",
        "decode_mode = \"topk\"  #@param [\"greedy\", \"nucleus\", \"topk\", \"beam\"] {type:\"string\"}\n",
        "k = 10  #@param {type:\"integer\"}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1U0dTSB-mnGN",
        "colab_type": "text"
      },
      "source": [
        "## Instala dependÃªncias\n",
        "\n",
        "- PyTorch Lightning\n",
        "- Hugginface Transformers\n",
        "- ERRANT (ERRor ANnotation Toolkit)\n",
        "- pyxDamerauLevenshtein"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OOXZxjWRkLMM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "try:\n",
        "    import pytorch_lightning\n",
        "    import transformers\n",
        "except ImportError as e:\n",
        "    # can't import modules, then install\n",
        "    !pip install --quiet pytorch-lightning\n",
        "    !pip install --quiet transformers\n",
        "    !pip install --quiet errant==2.0.0\n",
        "    !pip install pyxDamerauLevenshtein\n",
        "    !python -m spacy download en\n",
        "    # kill kernel (necessary for tqdm)\n",
        "    import os\n",
        "    os.kill(os.getpid(), 9)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ob7qL6kUVjbu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Importar todos os pacotes de uma sÃ³ vez para evitar duplicados ao longo do notebook.\n",
        "import datetime\n",
        "import errant\n",
        "import gzip\n",
        "import json\n",
        "import numpy as np\n",
        "import nvidia_smi\n",
        "import os\n",
        "import pandas as pd\n",
        "import joblib\n",
        "import psutil\n",
        "import pytorch_lightning as pl\n",
        "import random\n",
        "import spacy\n",
        "import sys\n",
        "import tarfile\n",
        "import tempfile\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from argparse import Namespace\n",
        "from google.colab import drive\n",
        "from itertools import cycle\n",
        "\n",
        "from pyxdameraulevenshtein import damerau_levenshtein_distance, \\\n",
        "    normalized_damerau_levenshtein_distance\n",
        "from pyxdameraulevenshtein import damerau_levenshtein_distance_ndarray, \\\n",
        "    normalized_damerau_levenshtein_distance_ndarray\n",
        "\n",
        "from pytorch_lightning.callbacks import ModelCheckpoint\n",
        "from pytorch_lightning.loggers import TensorBoardLogger\n",
        "from pytorch_lightning import Trainer\n",
        "\n",
        "from transformers import T5ForConditionalGeneration\n",
        "from transformers import T5Tokenizer\n",
        "from torch.optim import Adam\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "from typing import Dict\n",
        "from typing import List\n",
        "from typing import Tuple\n",
        "\n",
        "# Leard decoding solution\n",
        "import html\n",
        "import unicodedata\n",
        "\n",
        "nlp = spacy.load(\"en\")\n",
        "annotator = errant.load(\"en\", nlp)\n",
        "\n",
        "nvidia_smi.nvmlInit()\n",
        "handle = nvidia_smi.nvmlDeviceGetHandleByIndex(0)\n",
        "\n",
        "def hardware_stats():\n",
        "    \"\"\"\n",
        "    Returns a dict containing some hardware related stats\n",
        "    \"\"\"\n",
        "    res = nvidia_smi.nvmlDeviceGetUtilizationRates(handle)\n",
        "    return {\"cpu\": f\"{str(psutil.cpu_percent())}%\",\n",
        "            \"mem\": f\"{str(psutil.virtual_memory().percent)}%\",\n",
        "            \"gpu\": f\"{str(res.gpu)}%\",\n",
        "            \"gpu_mem\": f\"{str(res.memory)}%\"}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tm8H4Rfwm2gE",
        "colab_type": "text"
      },
      "source": [
        "## Define random seeds\n",
        "\n",
        "Importante: Fix seeds so we can replicate results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bJlZDb1VY29r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import random\n",
        "\n",
        "seed = 0\n",
        "random.seed(seed)\n",
        "torch.random.manual_seed(seed)\n",
        "torch.cuda.manual_seed(seed)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ETfkvMGl4JA1",
        "colab_type": "text"
      },
      "source": [
        "## Mapeia Google Drive\n",
        "\n",
        "Iremos salvar os checkpoints (pesos do modelo) no google drive, para que possamos continuar o treino de onde paramos."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "hvGjweSKfbA2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "330a6431-c86e-4803-fd92-77236c0cda40"
      },
      "source": [
        "drive.mount(\"/content/drive\")\n",
        "base_path = \"/content/drive/My Drive/PF-Correcao/bea2019st\"\n",
        "# base_path = \"/content/bea2019st\"\n",
        "os.environ[\"BASE_PATH\"] = base_path"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BMhp7hvN9mS6",
        "colab_type": "text"
      },
      "source": [
        "## ERRANT Scorer\n",
        "\n",
        "Comando para avaliaÃ§Ã£o que compara um arquivo M2 \"hipÃ³tese\" contra um arquivo M2 \"referÃªncia\".<br>\n",
        "\n",
        "### **Example**\n",
        "**Original**: This are gramamtical sentence .<br>\n",
        "**Corrected**: This is a grammatical sentence .<br>\n",
        "**Output M2**:<br>\n",
        "S This are gramamtical sentence .<br>\n",
        "A 1 2|||R:VERB:SVA|||is|||REQUIRED|||-NONE-|||0<br>\n",
        "A 2 2|||M:DET|||a|||REQUIRED|||-NONE-|||0<br>\n",
        "A 2 3|||R:SPELL|||grammatical|||REQUIRED|||-NONE-|||0<br>\n",
        "A -1 -1|||noop|||-NONE-|||REQUIRED|||-NONE-|||1<br>\n",
        "\n",
        "In M2 format, a line preceded by S denotes an original sentence while a line preceded by A indicates an edit annotation. Each edit line consists of the start and end token offset of the edit, the error type, and the tokenized correction string. The next two fields are included for historical reasons (see the CoNLL-2014 shared task) while the last field is the annotator id."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aWPWngE89ss_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f9e7785e-b7b0-4a17-b6a0-32f7d5281b3e"
      },
      "source": [
        "%%writefile orig.txt\n",
        "Eu nÃ£o cei pra ondi vou .\n",
        "Podi atÃ© num dÃ¡ em nada .\n",
        "Minha vida segui o sol .\n",
        "No horizonti dessa istrada ."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting orig.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dkdn9fTBS4lk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8a9ce06b-9ee9-412a-d764-6a2993578975"
      },
      "source": [
        "%%writefile ref.txt\n",
        "Eu nÃ£o sei pra onde vou .\n",
        "Pode atÃ© nÃ£o dar em nada .\n",
        "Minha vida segue o sol .\n",
        "No horizonte dessa estrada ."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting ref.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5eQ142HVSZu3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "008d8e86-74d5-4e2d-d5ad-88e2f204d951"
      },
      "source": [
        "%%writefile hyp.txt\n",
        "Eu nÃ£o sei pra ondi vou .\n",
        "Podi atÃ© nÃ£o dar em nada .\n",
        "Minha vida segui u sol .\n",
        "Num horizonte dessa estrada ."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting hyp.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a6KxhquG8U44",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!errant_parallel -orig orig.txt -cor ref.txt -out ref.m2 > /dev/null\n",
        "!errant_parallel -orig orig.txt -cor hyp.txt -out hyp.m2 > /dev/null"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qj0qyFWK9pI3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "a8da1274-93fd-4d69-bf2d-a8247f135590"
      },
      "source": [
        "import pandas as pd\n",
        "!errant_compare -hyp hyp.m2 -ref ref.m2\n",
        "# x = !errant_compare -hyp hyp.m2 -ref ref.m2\n",
        "# df = pd.DataFrame(data=x[2:4])[0].str.split('\\t', expand=True)\n",
        "# new_header = df.iloc[0] #grab the first row for the header\n",
        "# df = df[1:] #take the data less the header row\n",
        "# df.columns = new_header\n",
        "# df\n",
        "# df[\"F0.5\"][1]\n",
        "# d = df.apply(pd.to_numeric).to_dict('r')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "=========== Span-Based Correction ============\n",
            "TP\tFP\tFN\tPrec\tRec\tF0.5\n",
            "5\t2\t3\t0.7143\t0.625\t0.6944\n",
            "==============================================\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0W1V2fdg-RqU",
        "colab_type": "text"
      },
      "source": [
        "## Gerador \n",
        "\n",
        "From:\n",
        "https://github.com/huggingface/transformers/issues/3985"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YmxoZCZoIq6N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
        "# model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
        "# # Input text\n",
        "# original = 'This are gramamtical sentence .'\n",
        "# correct = 'This is a grammatical sentence .'\n",
        "\n",
        "# text = 'This <extra_id_0> sentence. </s>'\n",
        "\n",
        "# encoded = tokenizer.encode_plus(text, add_special_tokens=True, return_tensors='pt')\n",
        "# input_ids = encoded['input_ids']\n",
        "\n",
        "# # Generating 20 sequences with maximum length set to 5\n",
        "# outputs = model.generate(input_ids=input_ids, \n",
        "#                           num_beams=200, num_return_sequences=20,\n",
        "#                           max_length=5)\n",
        "\n",
        "# _0_index = text.index('<extra_id_0>')\n",
        "# _result_prefix = text[:_0_index]\n",
        "# _result_suffix = text[_0_index+12:]  # 12 is the length of <extra_id_0>\n",
        "\n",
        "# def _filter(output, end_token='<extra_id_1>'):\n",
        "#     # The first token is <unk> (inidex at 0) and the second token is <extra_id_0> (indexed at 32099)\n",
        "#     _txt = tokenizer.decode(output[2:], skip_special_tokens=False, clean_up_tokenization_spaces=False)\n",
        "#     if end_token in _txt:\n",
        "#         _end_token_index = _txt.index(end_token)\n",
        "#         return _result_prefix + _txt[:_end_token_index] + _result_suffix\n",
        "#     else:\n",
        "#         return _result_prefix + _txt + _result_suffix\n",
        "\n",
        "# results = list(map(_filter, outputs))\n",
        "# results\n",
        "# del tokenizer\n",
        "# del model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "TgUkD55Dkam2"
      },
      "source": [
        "## Classe Dataset\n",
        "Gerenciamento dos dados, e um pequeno teste."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "R2GaMtgRkaze",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 870
        },
        "outputId": "0dae61d4-3e9c-4f0a-ad89-80674cf5a094"
      },
      "source": [
        "hparams = {\"model_name\": model_name, \"seq_len\": sequence_length, \"batch_size\": batch_size}\n",
        "class BEA2019(Dataset):\n",
        "    \"\"\"\n",
        "    Loads data from preprocessed file and manages them.\n",
        "    \"\"\"\n",
        "    VALID_MODES = [\"train\", \"validation\", \"test\"]\n",
        "    TOKENIZER = T5Tokenizer.from_pretrained(hparams[\"model_name\"],\n",
        "                                            cache_dir=base_path)\n",
        "    def __init__(self, mode: str, seq_len: int):\n",
        "        \"\"\"\n",
        "        mode: One of train, validation or test \n",
        "        seq_len: limit to returned encoded tokens\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        assert mode in BEA2019.VALID_MODES\n",
        "\n",
        "        self.mode = mode\n",
        "        self.seq_len = seq_len\n",
        "\n",
        "        file_name = os.path.join(base_path, f\"{mode}.pkl\")\n",
        "        if not os.path.isfile(file_name):\n",
        "            print(\"Pre-processed files not found, preparing data.\")\n",
        "            self.prepare_data()\n",
        "        \n",
        "        with open(file_name, \"rb\") as preprocessed_file:\n",
        "            self.data = joblib.load(preprocessed_file)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, i: int):\n",
        "        \"\"\"\n",
        "        Unpacks line from data and applies T5 encoding if necessary.\n",
        "\n",
        "        returns: input (corrupted, original), target (corrected)\n",
        "        \"\"\"\n",
        "        input, target = self.data[i]\n",
        "        return input, target\n",
        "\n",
        "    def get_dataloader(self, batch_size: int, shuffle: bool):\n",
        "        return DataLoader(self, batch_size=batch_size, shuffle=shuffle, \n",
        "                          num_workers=4)\n",
        "\n",
        "    @staticmethod\n",
        "    def load_text_tuples(path, member):\n",
        "        \"\"\"\n",
        "        Load tuples from original files: text_original, text_corrected.\n",
        "        \"\"\"\n",
        "        text_tuples = []\n",
        "        with tarfile.open(path) as tar:\n",
        "            f = tar.extractfile(member)\n",
        "            for line in f:\n",
        "                data = json.loads(line)\n",
        "                # list of edits, one for each annotators\n",
        "                for annotator_id, edits in data[\"edits\"]:\n",
        "                    # edit: [annotator_id, [[char_start_offset, char_end_offset, correction], ...]]\n",
        "\n",
        "                    text_original = data[\"text\"]\n",
        "                    text_corrected = \"\"\n",
        "                    prev_char_end_offset = 0\n",
        "\n",
        "                    for idx, (char_start_offset, char_end_offset, correction) in enumerate(edits):\n",
        "                        # a slice of unchanged original text \n",
        "                        text_unchanged = text_original[prev_char_end_offset:char_start_offset]\n",
        "\n",
        "                        if correction is None:\n",
        "                            correction = \"\"\n",
        "                        text_corrected = f\"{text_corrected} {text_unchanged}{correction}\"\n",
        "\n",
        "                        prev_char_end_offset = char_end_offset\n",
        "\n",
        "                    text_unchanged = text_original[prev_char_end_offset:]\n",
        "                    text_corrected = f\"{text_corrected} {text_unchanged}\".lstrip()\n",
        "                    text_tuples.append((text_original, text_corrected))\n",
        "\n",
        "        return text_tuples\n",
        "\n",
        "    @staticmethod\n",
        "    def prepare_data(train_size=1000, val_size=300):\n",
        "        \"\"\"\n",
        "        Performs everything needed to get the data ready.\n",
        "        Addition of Eos token and encoding is performed in runtime.\n",
        "        \"\"\"\n",
        "        if not os.path.isfile(\"wi+locness_v2.1.bea19.tar.gz\"):    \n",
        "            !wget -nc https://www.cl.cam.ac.uk/research/nl/bea2019st/data/wi+locness_v2.1.bea19.tar.gz -P \"$BASE_PATH\"\n",
        "            # !wget -nc https://www.cl.cam.ac.uk/research/nl/bea2019st/data/ABCN.bea19.dev.orig -P \"$BASE_PATH\"\n",
        "            # !wget -nc https://www.cl.cam.ac.uk/research/nl/bea2019st/data/ABCN.bea19.test.orig -P \"$BASE_PATH\"\n",
        "\n",
        "        data = {}\n",
        "        train_val_data = BEA2019.load_text_tuples(os.path.join(base_path, \"wi+locness_v2.1.bea19.tar.gz\"), \"wi+locness/json/A.train.json\")\n",
        "        test_data = BEA2019.load_text_tuples(os.path.join(base_path, \"wi+locness_v2.1.bea19.tar.gz\"), \"wi+locness/json/A.dev.json\")\n",
        "\n",
        "        random.shuffle(train_val_data)\n",
        "\n",
        "        train_data = train_val_data[:train_size]\n",
        "        val_data = train_val_data[train_size:train_size + val_size]\n",
        "        test_data = test_data\n",
        "\n",
        "        for mode, data in zip(BEA2019.VALID_MODES, [train_data, val_data, test_data]):\n",
        "            file_name = os.path.join(base_path, f\"{mode}.pkl\")\n",
        "            with open(file_name, \"wb\") as pkl_file:\n",
        "                joblib.dump(data, pkl_file)\n",
        "            print(f\"Pre-processed data saved as {file_name}.\")\n",
        "\n",
        "\n",
        "datasets = {m: BEA2019(mode=m, seq_len=hparams[\"seq_len\"]) for m in BEA2019.VALID_MODES}\n",
        "\n",
        "# Testing datasets\n",
        "for mode, dataset in datasets.items():\n",
        "    print(f\"\\n{mode} dataset length: {len(dataset)}\\n\")\n",
        "    print(\"Random sample\")\n",
        "    input, target = random.choice(dataset)\n",
        "    print(\"input\\n\", input, end=\"\\n\\n\")\n",
        "    print(\"target\\n\", target, end=\"\\n\\n\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "train dataset length: 1000\n",
            "\n",
            "Random sample\n",
            "input\n",
            " At present, many people think that English is the world language and it is absolutely an essential skill to communicate with other countries people. Due to this reason , Taiwanese parents do not want their kids to start behind others, thus, many kindergartens started to teach children English.\n",
            "\n",
            "\n",
            "target\n",
            " At present, many people think that English is the world language and it is absolutely an essential skill to communicate with people from other countries . For  this reason,  Taiwanese parents do not want their kids to be  behind others. Therefore , many kindergartens have  started to teach children English.\n",
            "\n",
            "\n",
            "\n",
            "validation dataset length: 300\n",
            "\n",
            "Random sample\n",
            "input\n",
            " No future for public transport?\n",
            "why do use public transport some of us use it to save money for not buying gas or they can't buy a car or if they had problems in their cars or they don't have the mood to drive.\n",
            "Now a days with technology we have different ways to transport like hoverboards,skateboards and maybe it makes No future for public transport \n",
            "Another point is we have our own cars and it's more comfortable, I don't have to wait in line to buy tickets. I don't have to be late for the delays, I can do anything in my own car I can put music I can drink,eat,speak or wherever.\n",
            "I don't have really an opinion I just don't know.we will wait and find out.Tell me your opinions about this subject in comments.\n",
            "\n",
            "target\n",
            " No future for public transport?\n",
            "Why  do we  use public transport? Some  of us use it to save money by  not buying gas or because  they can't buy a car or if they have  problems with  their cars or they are n't in  the mood to drive.\n",
            "Nowadays,  with technology,  we have different ways to get around,  like hoverboards, skateboards,  and maybe it means there is  no  future for public transport.  \n",
            "Another point is that  we have our own cars and it's more comfortable.  I don't have to wait in line to buy tickets. I don't have to be late because of  the delays.  I can do anything in my own car.  I can put music on,  I can drink, eat, speak  or wherever.\n",
            "I don't really have  an opinion,  I just don't know. We  will have to  wait and find out.  Tell me your opinions on  this subject in comments.\n",
            "\n",
            "\n",
            "test dataset length: 130\n",
            "\n",
            "Random sample\n",
            "input\n",
            " \n",
            "Is is commonly-debated that tourism has greatly influenced,not only the aspects of one country,but of the whole world along. \n",
            "Whethere tourism has had a positive or a negative impact over our lifes,it remains quite a dilemma for the ignorants. \n",
            "First of all,tourism is a tool that enables people to travel all around the world.Alongside with its development,the ablity of travalling has easened to such a scale that it is now quite common to commute from one country to another.The existence of multinaionals is tightly connected to the idea of tourism,as well as with the idea of globalisation,since a traveller is not only a citizen of his own country but a global citizen.\n",
            "Secondly,tourism,especially in developed countries,has played an important part in their growing from an economic point of view.There are countries,such as Greece or Bulgaria,in which the econmoy relies merely on tourism.If tourism influences the econmy,it thereby influnces the environment,and if it influences the environment it influences the transport.How?people become more careful at their historcal sites,this way preserving them;Transport is developed both at a small and a large scale:At a small scale,in cities, in a way which will allow citizens and tourists alike to reach important places more efficently.At a large scale,people can now travel in almost any possible way:on land,on sea or air.Through the development of tourism were born cruises and train vacations.\n",
            "\n",
            "\n",
            "target\n",
            " It  is commonly debated  that tourism has greatly influenced  not only the aspects of a  country,  but of the whole world  . \n",
            "Whether  tourism has had a positive or a negative impact on  our lives ,  it remains quite a dilemma for  . \n",
            "First of all,  tourism is a tool that enables people to travel all around the world.  Alongside   its development,  the ability  to  travel  has become easier  to such an  extent  that it is now quite common to commute from one country to another.The existence of multinationals  is tightly connected to the idea of tourism,  as well as to  the idea of globalisation,  since a traveller is not just  a citizen of his own country,  but a global citizen.\n",
            "Secondly,  tourism,  especially in developed countries,  has played an important part in their growth  from an economic point of view.  There are countries,  such as Greece or Bulgaria,  in which the economy  relies completely  on tourism.  If tourism influences the economy ,  it thereby influences  the environment,  and if it influences the environment,  it influences   transport.  How?  People  become more careful at their historical  sites,  thereby  preserving them.  Transport is developed both on  a small and a large scale.  On  a small scale,  in cities, in a way which will allow citizens and tourists alike to reach important places more efficiently .  On  a large scale,  people can now travel in almost any possible way:  on land,  on sea or air.  Through the development of tourism cruises and train vacations were born .\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cloyt0tIwIiD",
        "colab_type": "text"
      },
      "source": [
        "## Dataloaders\n",
        "\n",
        "VerificaÃ§Ã£o se dataloaders estÃ£o funcionando corretamente."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZoKiQXCvwGrP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "4fcb3ba2-1be5-4348-a8d3-e12d2ce8a90d"
      },
      "source": [
        "shuffle = {\"train\": True, \"validation\": False, \"test\": False}\n",
        "debug_dataloaders = {mode: datasets[mode].get_dataloader(batch_size=hparams[\"batch_size\"], \n",
        "                                                         shuffle=shuffle[mode])\n",
        "                     for mode in BEA2019.VALID_MODES}\n",
        "\n",
        "# Testing dataloaders\n",
        "for mode, dataloader in debug_dataloaders.items():\n",
        "    print(\"{} number of batches: {}\".format(mode, len(dataloader)))\n",
        "    batch = next(iter(dataloader))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train number of batches: 100\n",
            "validation number of batches: 30\n",
            "test number of batches: 13\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YGH2rc3lthSB",
        "colab_type": "text"
      },
      "source": [
        "## Lightning Module\n",
        "\n",
        "Aqui a classe principal do PyTorch Lightning Ã© definida.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jg-hZEktbvnr",
        "colab": {}
      },
      "source": [
        "class T5Corrector(pl.LightningModule):\n",
        "    def __init__(self, hparams):\n",
        "        super().__init__()\n",
        "\n",
        "        self.hparams = hparams\n",
        "        self.t5 = T5ForConditionalGeneration.from_pretrained(hparams.model_name,\n",
        "                                                             cache_dir=hparams.base_path)\n",
        "        self.tokenizer = BEA2019.TOKENIZER\n",
        "        self.start_token = BEA2019.TOKENIZER.convert_tokens_to_ids('<extra_id_0>')\n",
        "        self.end_token = BEA2019.TOKENIZER.convert_tokens_to_ids('<extra_id_1>')\n",
        "\n",
        "    def select_correction(self, word, hypotheses):\n",
        "        \"\"\"\n",
        "        Selects the most probable correction for a given word and given hypotheses.\n",
        "        \"\"\"\n",
        "        if word in hypotheses:\n",
        "            # print('Only copy:', word)\n",
        "            return word\n",
        "        else:\n",
        "            distances = damerau_levenshtein_distance_ndarray(word, np.array(hypotheses))\n",
        "            # print(distances.shape)\n",
        "            # print('distances', distances)\n",
        "            idx_min_distance = np.argmin(distances)\n",
        "            # print('idx_min_distance', idx_min_distance)\n",
        "            # print('idx_min_distance distance', distances[idx_min_distance])\n",
        "            # print('idx_min_distance', hypotheses[idx_min_distance])\n",
        "            if distances[idx_min_distance] < 10:\n",
        "                # print('Replaced:', word, 'by', hypotheses[idx_min_distance])\n",
        "                return hypotheses[idx_min_distance]\n",
        "        # print('No suggestion, word copied:', word)\n",
        "        return word\n",
        "\n",
        "    def generate(self, original, end_token=\"<extra_id_1>\"):\n",
        "        \"\"\"\n",
        "        Generates a correction hypothesis for a given sentence.\n",
        "        \"\"\"\n",
        "        hypothesis = \"\"\n",
        "        words = original.split()\n",
        "        # we could not fit n masked_sentences in memory:\n",
        "        # masked_ids = []\n",
        "        # input_ids = torch.stack(masked_ids).squeeze().to(\"cuda\")\n",
        "        # masked_ids.append(input_ids)\n",
        "\n",
        "        for idx, word in enumerate(words):\n",
        "            # masks the i-th word\n",
        "            input = f\"{words[:idx]} <extra_id_0> {words[idx + 1:]} {self.tokenizer.eos_token}\"\n",
        "            input_ids = self.tokenizer.encode(input,\n",
        "                                              max_length=self.hparams.seq_len,\n",
        "                                              pad_to_max_length=True,\n",
        "                                              add_special_tokens=True,\n",
        "                                              return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "            # generates k hypothesis for the sentence\n",
        "            hypotheses = self.t5.generate(input_ids=input_ids,\n",
        "                                        top_k=self.hparams.k,\n",
        "                                        do_sample=True,\n",
        "                                        num_return_sequences=self.hparams.k)\n",
        "            hypotheses = [self.tokenizer.decode(output_ids[2:],\n",
        "                                                skip_special_tokens=False,\n",
        "                                                clean_up_tokenization_spaces=False) for output_ids in hypotheses]\n",
        "\n",
        "            hypothesis_word = self.select_correction(word, hypotheses)\n",
        "            hypothesis = f\"{hypothesis} {hypothesis_word}\"\n",
        "\n",
        "        return hypothesis\n",
        "\n",
        "    def forward(self, x):\n",
        "        inputs, targets = x\n",
        "\n",
        "        if self.training:\n",
        "            input_ids = []\n",
        "            attention_mask = []\n",
        "            lm_labels = []\n",
        "            # for each sample in batch\n",
        "            for input, target in zip(inputs, targets):\n",
        "                input = f\"{input} {self.tokenizer.eos_token}\"\n",
        "                d = self.tokenizer.encode_plus(\n",
        "                    input,\n",
        "                    max_length=self.hparams.seq_len,\n",
        "                    pad_to_max_length=True,\n",
        "                    add_special_tokens=True,\n",
        "                    return_tensors=\"pt\").to(\"cuda\")\n",
        "                input_ids.append(d[\"input_ids\"])\n",
        "                attention_mask.append(d[\"attention_mask\"])\n",
        "                lm_labels.append(self.tokenizer.encode(\n",
        "                    target,\n",
        "                    max_length=self.hparams.seq_len,\n",
        "                    pad_to_max_length=True,\n",
        "                    add_special_tokens=True,\n",
        "                    return_tensors=\"pt\"))\n",
        "\n",
        "            input_ids = torch.stack(input_ids).squeeze(1).to(\"cuda\")\n",
        "            attention_mask = torch.stack(attention_mask).squeeze(1).to(\"cuda\")\n",
        "            lm_labels = torch.stack(lm_labels).squeeze(1).to(\"cuda\")\n",
        "\n",
        "            outputs = self.t5(input_ids=input_ids, \n",
        "                              attention_mask=attention_mask,\n",
        "                              lm_labels=lm_labels)\n",
        "            loss, predicted_scores = outputs[:2]\n",
        "            return loss, predicted_scores, inputs, targets\n",
        "        else:\n",
        "            predicts = []\n",
        "            # for each sample in batch\n",
        "            for input, target in zip(inputs, targets):\n",
        "                predicts.append(self.generate(input))\n",
        "                # print(\"original\", orig)\n",
        "                # print(\"corrected\", corr)\n",
        "                # print(\"outputs\", outputs[-1])\n",
        "            return predicts, inputs, targets\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        loss, predicted_scores, inputs, targets = self(batch)\n",
        "\n",
        "        return {\"loss\": loss, \"log\": {\"loss\": loss}, \"progress_bar\": hardware_stats()}\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        predicts, inputs, targets = self(batch)\n",
        "\n",
        "        with open(\"orig.txt\", \"w\") as f:\n",
        "            for input in inputs:\n",
        "                input = input.replace(\"\\n\", \"\")\n",
        "                f.write(f\"{input}\\n\")\n",
        "\n",
        "        with open(\"ref.txt\", \"w\") as f:\n",
        "            for target in targets:\n",
        "                target = target.replace(\"\\n\", \"\")\n",
        "                f.write(f\"{target}\\n\")\n",
        "\n",
        "        with open(\"hyp.txt\", \"w\") as f:\n",
        "            for pred in predicts:\n",
        "                pred = pred.replace(\"\\n\", \"\")\n",
        "                f.write(f\"{pred}\\n\")\n",
        "\n",
        "        !errant_parallel -orig orig.txt -cor ref.txt -out ref.m2 > /dev/null\n",
        "        !errant_parallel -orig orig.txt -cor hyp.txt -out hyp.m2 > /dev/null\n",
        "        x = !errant_compare -hyp hyp.m2 -ref ref.m2\n",
        "        df = pd.DataFrame(data=x[2:4])[0].str.split(\"\\t\", expand=True)\n",
        "        new_header = df.iloc[0] #grab the first row for the header\n",
        "        df = df[1:].apply(pd.to_numeric) #take the data less the header row\n",
        "        df.columns = new_header\n",
        "\n",
        "        true_positive = df[\"TP\"][1]\n",
        "        false_positive = df[\"FP\"][1]\n",
        "        false_negative = df[\"FN\"][1]\n",
        "        precision = df[\"Prec\"][1]\n",
        "        recall = df[\"Rec\"][1]\n",
        "        f_score = df[\"F0.5\"][1]\n",
        "\n",
        "        progress_bar = hardware_stats()\n",
        "        progress_bar.update({\"precision\": precision, \"recall\": recall, \"f_score\": f_score})\n",
        "        # print(\"progress_bar\", progress_bar)\n",
        "\n",
        "        return {\"true_positive\": true_positive, \"false_positive\": false_positive, \"false_negative\": false_negative,\n",
        "                \"precision\": precision, \"recall\": recall, \"f_score\": f_score,\n",
        "                \"predicts\": predicts, \"inputs\": inputs, \"targets\": targets, \"progress_bar\": progress_bar}\n",
        "\n",
        "    def test_step(self, batch, batch_idx):\n",
        "        predicts, inputs, targets = self(batch)\n",
        "\n",
        "        with open(\"orig.txt\", \"w\") as f:\n",
        "            for input in inputs:\n",
        "                input = input.replace(\"\\n\", \"\")\n",
        "                f.write(f\"{input}\\n\")\n",
        "\n",
        "        with open(\"ref.txt\", \"w\") as f:\n",
        "            for target in targets:\n",
        "                target = target.replace(\"\\n\", \"\")\n",
        "                f.write(f\"{target}\\n\")\n",
        "\n",
        "        with open(\"hyp.txt\", \"w\") as f:\n",
        "            for pred in predicts:\n",
        "                pred = pred.replace(\"\\n\", \"\")\n",
        "                f.write(f\"{pred}\\n\")\n",
        "\n",
        "        !errant_parallel -orig orig.txt -cor ref.txt -out ref.m2 > /dev/null\n",
        "        !errant_parallel -orig orig.txt -cor hyp.txt -out hyp.m2 > /dev/null\n",
        "        x = !errant_compare -hyp hyp.m2 -ref ref.m2\n",
        "        df = pd.DataFrame(data=x[2:4])[0].str.split(\"\\t\", expand=True)\n",
        "        new_header = df.iloc[0] #grab the first row for the header\n",
        "        df = df[1:].apply(pd.to_numeric) #take the data less the header row\n",
        "        df.columns = new_header\n",
        "\n",
        "        true_positive = df[\"TP\"][1]\n",
        "        false_positive = df[\"FP\"][1]\n",
        "        false_negative = df[\"FN\"][1]\n",
        "        precision = df[\"Prec\"][1]\n",
        "        recall = df[\"Rec\"][1]\n",
        "        f_score = df[\"F0.5\"][1]\n",
        "\n",
        "        progress_bar = hardware_stats()\n",
        "        progress_bar.update({\"precision\": precision, \"recall\": recall, \"f_score\": f_score})\n",
        "        # print(\"progress_bar\", progress_bar)\n",
        "\n",
        "        return {\"true_positive\": true_positive, \"false_positive\": false_positive, \"false_negative\": false_negative,\n",
        "                \"precision\": precision, \"recall\": recall, \"f_score\": f_score,\n",
        "                \"predicts\": predicts, \"inputs\": inputs, \"targets\": targets, \"progress_bar\": progress_bar}\n",
        "\n",
        "    def training_epoch_end(self, outputs):\n",
        "        avg_loss = torch.stack([x[\"loss\"] for x in outputs]).mean()\n",
        "\n",
        "        return {\"log\": {\"train_loss\": avg_loss}} \n",
        "\n",
        "    def validation_epoch_end(self, outputs):\n",
        "        avg_precision = sum([x[\"precision\"] for x in outputs]) / len(outputs)\n",
        "        avg_recall = sum([x[\"recall\"] for x in outputs]) / len(outputs)\n",
        "        avg_f_score = sum([x[\"f_score\"] for x in outputs]) / len(outputs)\n",
        "\n",
        "        tensorboard_logs = {\"avg_precision\": avg_precision,\n",
        "                            \"avg_recall\": avg_recall,\n",
        "                            \"avg_f_score\": avg_f_score}\n",
        "\n",
        "        origs = sum([list(x[\"inputs\"]) for x in outputs], [])\n",
        "        trues = sum([list(x[\"targets\"]) for x in outputs], [])\n",
        "        preds = sum([list(x[\"predicts\"]) for x in outputs], [])\n",
        "\n",
        "        n = random.choice(range(len(trues)))\n",
        "        print(f\"\\Input: {origs[n]}\\nTarget: {trues[n]}\\nPrediction: {preds[n]}\\n\")\n",
        "\n",
        "        return {\"avg_precision\": avg_precision, \"avg_recall\": avg_recall, \"avg_f_score\": avg_f_score,\n",
        "                \"log\": tensorboard_logs, \"progress_bar\": tensorboard_logs}\n",
        "\n",
        "    def test_epoch_end(self, outputs):\n",
        "        avg_precision = sum([x[\"precision\"] for x in outputs]) / len(outputs)\n",
        "        avg_recall = sum([x[\"recall\"] for x in outputs]) / len(outputs)\n",
        "        avg_f_score = sum([x[\"f_score\"] for x in outputs]) / len(outputs)\n",
        "\n",
        "        tensorboard_logs = {\"avg_precision\": avg_precision,\n",
        "                            \"avg_recall\": avg_recall,\n",
        "                            \"avg_f_score\": avg_f_score}\n",
        "\n",
        "        origs = sum([list(x[\"inputs\"]) for x in outputs], [])\n",
        "        trues = sum([list(x[\"targets\"]) for x in outputs], [])\n",
        "        preds = sum([list(x[\"predicts\"]) for x in outputs], [])\n",
        "\n",
        "        n = random.choice(range(len(trues)))\n",
        "        print(f\"\\Input: {origs[n]}\\nTarget: {trues[n]}\\nPrediction: {preds[n]}\\n\")\n",
        "        \n",
        "        return {\"avg_precision\": avg_precision, \"avg_recall\": avg_recall, \"avg_f_score\": avg_f_score,\n",
        "                \"log\": tensorboard_logs, \"progress_bar\": tensorboard_logs}\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        return Adam(self.parameters(), lr=self.hparams.lr)    \n",
        "\n",
        "    def train_dataloader(self):\n",
        "        if self.hparams.overfit_pct > 0:\n",
        "            logging.info(\"Disabling train shuffle due to overfit_pct.\")\n",
        "            shuffle = False\n",
        "        else:\n",
        "            shuffle = True\n",
        "        dataset = BEA2019(\"train\", seq_len=self.hparams.seq_len)\n",
        "        return dataset.get_dataloader(batch_size=self.hparams.batch_size, shuffle=shuffle)\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        dataset = BEA2019(\"validation\", seq_len=self.hparams.seq_len)\n",
        "        return dataset.get_dataloader(batch_size=self.hparams.batch_size, shuffle=False)\n",
        "\n",
        "    def test_dataloader(self):\n",
        "        dataset = BEA2019(\"test\", seq_len=self.hparams.seq_len)\n",
        "        return dataset.get_dataloader(batch_size=self.hparams.batch_size, shuffle=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "YbA8e334UM-N"
      },
      "source": [
        "## PreparaÃ§Ã£o"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "XDAYv5_zURPc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "ab9382fc-3157-40e6-e3e3-510df70fa6e3"
      },
      "source": [
        "hparams = {\"name\": experiment_name, \"base_path\": base_path,\n",
        "           \"model_name\": model_name, \"seq_len\": sequence_length,\n",
        "           \"decode_mode\": decode_mode, \"k\": k,\n",
        "           \"lr\": learning_rate, \"batch_size\": batch_size, \"batch_accum\": accumulate_grad_batches,\n",
        "           \"max_epochs\": 3,\n",
        "           \"overfit_pct\": 0, \"debug\": 0,\n",
        "           \"decode_mode\": decode_mode}\n",
        "\n",
        "\n",
        "for key, parameter in hparams.items():\n",
        "    print(\"{}: {}\".format(key, parameter))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "name: no-tuning\n",
            "base_path: /content/drive/My Drive/PF-Correcao/bea2019st\n",
            "model_name: t5-small\n",
            "seq_len: 1000\n",
            "decode_mode: topk\n",
            "k: 10\n",
            "lr: 0.005\n",
            "batch_size: 10\n",
            "batch_accum: 1\n",
            "max_epochs: 3\n",
            "overfit_pct: 0\n",
            "debug: 0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fsxBZdkDzhNQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "09051d13-3272-4b51-d153-8f24a3b8fc60"
      },
      "source": [
        "# Instantiate model\n",
        "model = T5Corrector(Namespace(**hparams))\n",
        "\n",
        "# Folder/path management, for logs and checkpoints\n",
        "tensorboard_path = os.path.join(base_path, \"logs\")\n",
        "experiment_name = hparams[\"name\"]\n",
        "model_folder = os.path.join(tensorboard_path, experiment_name)\n",
        "os.makedirs(model_folder, exist_ok=True)\n",
        "ckpt_path = os.path.join(model_folder, \"-{epoch}\")\n",
        "\n",
        "# Callback initialization\n",
        "checkpoint_callback = ModelCheckpoint(prefix=experiment_name, \n",
        "                                      filepath=ckpt_path, \n",
        "                                      mode=\"max\")\n",
        "logger = TensorBoardLogger(tensorboard_path, experiment_name)\n",
        "\n",
        "# PL Trainer initialization\n",
        "trainer = Trainer(gpus=1,\n",
        "                  checkpoint_callback=checkpoint_callback, \n",
        "                  early_stop_callback=False,\n",
        "                  logger=logger,\n",
        "                  accumulate_grad_batches=hparams[\"batch_accum\"],\n",
        "                  max_epochs=hparams[\"max_epochs\"], \n",
        "                  fast_dev_run=bool(hparams[\"debug\"]), \n",
        "                  overfit_pct=hparams[\"overfit_pct\"],\n",
        "                  progress_bar_refresh_rate=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GPU available: True, used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "CUDA_VISIBLE_DEVICES: [0]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gcCLkmIvmrx6",
        "colab_type": "text"
      },
      "source": [
        "## Tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T56srWEzmttv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 838
        },
        "outputId": "90053da3-0753-4f63-92a3-76e2b8a58832"
      },
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir \"/content/drive/My Drive/PF-Correcao/bea2019st\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Reusing TensorBoard on port 6006 (pid 1111), started 0:30:17 ago. (Use '!kill 1111' to kill it.)"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "        (async () => {\n",
              "            const url = await google.colab.kernel.proxyPort(6006, {\"cache\": true});\n",
              "            const iframe = document.createElement('iframe');\n",
              "            iframe.src = url;\n",
              "            iframe.setAttribute('width', '100%');\n",
              "            iframe.setAttribute('height', '800');\n",
              "            iframe.setAttribute('frameborder', 0);\n",
              "            document.body.appendChild(iframe);\n",
              "        })();\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aslaho6VpMSL",
        "colab_type": "text"
      },
      "source": [
        "## Teste"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "efVSbZD1-chp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 256,
          "referenced_widgets": [
            "bb99515e171745f3a765322dd58b8a76",
            "05127b4eb5154f2888529d301561265b",
            "9dad062bfea649399e92e4539d6acc30",
            "9ea6d193ff5f415e82fe38141574ebd4",
            "edc02f4530d14289aeeb88283df12550",
            "1650921dad654f9abbc277741e643f9f",
            "b5efa7f90bee4c3f887587f96ddd9c0d",
            "0f9f83fc2e7a4d0aa305c18fc25d3f5f"
          ]
        },
        "outputId": "c79da85a-b90d-4094-ce09-7ae1ba4668cf"
      },
      "source": [
        "trainer.test(model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bb99515e171745f3a765322dd58b8a76",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Testing', layout=Layout(flex='2'), max=â€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\\Input: Michael was a little kid when he had a dream that was: Be a super hero! After many years he still dream to become a super hero. He enter  the university of medicine because he thinks that this profession was the more similar to be a super hero. Pass some years  of the university and he know a girl called Kate and he get loved on here and she get loved in him. Kate was cursing the university of fashion. Then the two started to date. Michael and Kate was so  happy, until one day that Michael said to him dream to be a super hero, Kate get so nervous saying that it was ridiculous and just a kid dream and that Michael was only dreaming but that would never happen and that super hero donâ€™t exist. When she said that, Michael started to cry and get mad saying that she was lying and donÂ´t have heart. He got so mad that he asked her to  get out of his apartment. In following day, she said sorry and them was happy again. But when them get to the home of Michael they fight again and Michael get a knife and kill her. He cried with a lot  blood around. Michael get away from there. Michael closed the door and knew at that moment he had made a mistake.\n",
            "Target: Michael was a little kid when he had a dream that was: to  be  a super hero! After many years,  he still dreamt  of  becoming  a super hero. He went to   university to study  medicine because he thought  that that  profession was the most  similar to being  a super hero. After  some years  at   university  he met  a girl called Kate and he fell in  love  with  her  and she fell in  love  with  him. Kate was attending   university to study  fashion. Then the two started to date. Michael and Kate were  so  happy, until one day  Michael told   her about his  dream of  being  a super hero.  Kate got  so agitated,  saying that it was ridiculous and just a kid's  dream and that Michael was only dreaming and  that would never happen and that super heroes  did nâ€™t exist. When she said that, Michael started to cry and got  mad,  saying that she was lying and didnÂ´t  have a  heart. He got so mad that he asked her to  get out of his apartment. The  following day, she said sorry and they  were  happy again. But when they  got  to Michael's house,  they fought  again and Michael got  a knife and kill her. He cried with a lot of  blood around. Michael got  away from there. Michael closed the door and knew at that moment he had made a mistake.\n",
            "Prediction:  Michael was a little kid when he had a dream that was: Be a super hero! After many years he still dream to become a super hero. He enter the university of medicine because he thinks that this profession was the more similar to be a super hero. Pass some years of the university and he know a girl called Kate and he get loved on here and she get loved in him. Kate was cursing the university of fashion. Then the two started to date. Michael and Kate was so happy, until one day that Michael said to him dream to be a super hero, Kate get so nervous saying that it was ridiculous and just a kid dream and that Michael was only dreaming but that would never happen and that super hero donâ€™t exist. When she said that, Michael started to cry and get mad saying that she was lying and donÂ´t have heart. He got so mad that he asked her to get out of his apartment. In following day, she said sorry and them was happy again. But when them get to the home of Michael they fight again and Michael get a knife and kill her. He cried with a lot blood around. Michael get away from there. Michael closed the door and knew at that moment he had made a mistake.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "TEST RESULTS\n",
            "{'avg_f_score': 0.0023615384615384614,\n",
            " 'avg_precision': 0.011046153846153846,\n",
            " 'avg_recall': 0.0005692307692307692}\n",
            "--------------------------------------------------------------------------------\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6TYZSh-UvOJS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}